{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/robsalgado/personal_data_science_projects/blob/master/topic_modeling_nmf/nlp_topic_utils.ipynb\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contraction map\n",
    "c_dict = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you you will\",\n",
    "    \"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Compiling the contraction dict\n",
    "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
    "\n",
    "# List of stop words\n",
    "add_stop = ['said', 'say', '...', 'like', 'ad', 'ha', 'wa', 'reuters', 'just', 'cap']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
    "\n",
    "# List of punctuation\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "# Splits words on white spaces (leaves contractions intact) and splits out\n",
    "# trailing punctuation\n",
    "def casual_tokenizer(text):\n",
    "    \"\"\" Called in process_text. \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, c_re=c_re):\n",
    "    \"\"\" Called in process_text. \"\"\"\n",
    "    def replace(match):\n",
    "        return c_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def process_text(text):\n",
    "    text = casual_tokenizer(text)\n",
    "    text = [each.lower() for each in text]\n",
    "    text = [expand_contractions(each, c_re=c_re) for each in text]\n",
    "    text = [re.sub('[0-9]+', '', each) for each in text]\n",
    "    text = [re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", each) for each in text]\n",
    "    text = [re.sub('[^a-zA-Z]', ' ', each) for each in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(each) for each in text]\n",
    "    text = [w for w in text if w not in punc]\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [each for each in text if len(each) > 1]\n",
    "    text = [each for each in text if ' ' not in each]\n",
    "    # text = unique_words(text)\n",
    "    return text\n",
    "\n",
    "def top_words(topic, n_top_words):\n",
    "    return topic.argsort()[:-n_top_words - 1:-1]  \n",
    "\n",
    "def topic_table(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        t = (topic_idx)\n",
    "        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n",
    "    return pd.DataFrame(topics)\n",
    "\n",
    "def whitespace_tokenizer(text): \n",
    "    pattern = r\"(?u)\\b\\w\\w+\\b\" \n",
    "    tokenizer_regex = RegexpTokenizer(pattern)\n",
    "    tokens = tokenizer_regex.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Funtion to remove duplicate words\n",
    "def unique_words(text): \n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in text if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "def word_count(text):\n",
    "    return len(str(text).split(' '))\n",
    "\n",
    "def replicate_csv(data):\n",
    "    # recreating csv from modeling_ii notebook\n",
    "    data['tokenized_essay'] = data.essay.apply(process_text)\n",
    "    data['word_count'] = data.essay.apply(word_count)\n",
    "    data = data.fillna(0)\n",
    "    data['max_score'] = 0\n",
    "    essay_sets = data.essay_set.unique()\n",
    "    for set_ in essay_sets:\n",
    "        if set_ == 1:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 12\n",
    "        if set_ == 2:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 10\n",
    "        if set_ == 3 or set_ == 4:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 3\n",
    "        if set_ == 5 or set_ == 6:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 4\n",
    "        if set_ == 7:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 30\n",
    "        if set_ == 8:\n",
    "            data.loc[data.essay_set == set_, 'max_score'] = 60\n",
    "    data['pct_score'] = 0\n",
    "    for set_ in essay_sets:\n",
    "        if set_ == 2:\n",
    "            data.loc[data.essay_set == set_, 'pct_score'] = (data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                       + data.loc[data.essay_set==set_,'domain2_score']) \\\n",
    "                                                       / data.loc[data.essay_set==set_,'max_score']\n",
    "            continue\n",
    "        else:\n",
    "            data.loc[data.essay_set == set_, 'pct_score'] = data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                       / data.loc[data.essay_set==set_,'max_score']\n",
    "    data['class'] = 1\n",
    "    for x in range(len(data)):\n",
    "        if (data.pct_score[x]) >= .9:\n",
    "            data['class'][x] = 5\n",
    "            continue\n",
    "        elif data.pct_score[x] >= .8 and data.pct_score[x] < .9:\n",
    "            data['class'][x] = 4\n",
    "            continue\n",
    "        elif data.pct_score[x] >= .7 and data.pct_score[x] < .8:\n",
    "            data['class'][x] = 3\n",
    "            continue\n",
    "        elif data.pct_score[x] >= .6 and data.pct_score[x] < .7:\n",
    "            data['class'][x] = 2\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    try:\n",
    "        from gensim.models import KeyedVectors\n",
    "        model = KeyedVectors.load('mywordvecs.kvmodel')\n",
    "        return model\n",
    "    except:\n",
    "        import gensim.downloader as api\n",
    "        model = api.load(\"glove-wiki-gigaword-300\")\n",
    "        model.save('mywordvecs.kvmodel')\n",
    "        return model\n",
    "\n",
    "def reduce_words(arr, model, reductions=5):\n",
    "    \"\"\"combine list of words into the one most similar label\"\"\"\n",
    "    if not model:\n",
    "        model = get_model()\n",
    "    arr_copy = arr.copy()\n",
    "    neg = []\n",
    "    for i in range(reductions):\n",
    "        try:\n",
    "            rem = model.doesnt_match(arr_copy)\n",
    "            # neg.append(rem)\n",
    "            arr_copy.remove(rem)\n",
    "        except:\n",
    "            pass\n",
    "    # result = model.most_similar_cosmul(positive=arr, negative=neg, topn=10)\n",
    "    try:\n",
    "        result = model.most_similar(positive=arr_copy, negative=neg)\n",
    "    except KeyError as e:\n",
    "        # unrecognized key, usually a typo or some proper noun\n",
    "        text = str(e)\n",
    "        # it's given in single quotes, so find that word\n",
    "        pattern = r\"'([A-Za-z0-9_\\./\\\\-]*)'\"\n",
    "        m = re.search(pattern, text)\n",
    "        kw = str(m.group()).replace(\"'\", \"\")\n",
    "        arr_copy.remove(kw)\n",
    "        arr.remove(kw)\n",
    "        try:\n",
    "            result = model.most_similar(positive=arr_copy, negative=neg)\n",
    "        except:\n",
    "            # this block returns a single string, \n",
    "            # which is the reason for the \n",
    "            # isinstance call below\n",
    "            result = reduce_words(arr, model, reductions=reductions-1)\n",
    "    except:\n",
    "        # sometimes the reduction removes all the words from the list\n",
    "        # so we re-run with the original list and remove the negative words\n",
    "        result = model.most_similar(positive=arr, negative=[])\n",
    "    # print('result:', result)\n",
    "    if isinstance(result, list):\n",
    "        most_similar_key, similarity = result[0]  # look at the first match\n",
    "    else:\n",
    "        most_similar_key = result\n",
    "    # print(f\"{most_similar_key}: {similarity:.4f}\")\n",
    "    return most_similar_key\n",
    "\n",
    "def generate_para_topics(df):\n",
    "    \"\"\" Takes a tokenized paragraph and labels it with a topic. \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    # use or build w2v model\n",
    "    model = get_model()\n",
    "    tokenized_text = df['token_p']\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "    no_features = 1000\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.85, \n",
    "                                    min_df=1, \n",
    "                                    max_features=no_features, \n",
    "                                    stop_words='english', \n",
    "                                    preprocessor=' '.join)\n",
    "    tf = tf_vectorizer.fit_transform(tokenized_text) # tf embeddings\n",
    "    # print(\"tf: \\n\", tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    n_components = 10\n",
    "\n",
    "    # Run LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5, \n",
    "                                    learning_method='online', learning_offset=50.,\n",
    "                                    random_state=0).fit(tf)\n",
    "\n",
    "    lda_docweights = lda.transform(tf_vectorizer.transform(tokenized_text))\n",
    "\n",
    "    n_top_words = 10\n",
    "\n",
    "    lda_topic_df = topic_table(\n",
    "        lda,\n",
    "        tf_feature_names,\n",
    "        n_top_words\n",
    "    ).T\n",
    "\n",
    "    # Cleaning up the top words to create topic summaries\n",
    "    lda_topic_df['topics'] = lda_topic_df.apply(lambda x: [' '.join(x)], axis=1) # Joining each word into a list\n",
    "    lda_topic_df['topics'] = lda_topic_df['topics'].str[0]  # Removing the list brackets\n",
    "    lda_topic_df['topics'] = lda_topic_df['topics'].apply(lambda x: whitespace_tokenizer(x)) # tokenize\n",
    "    lda_topic_df['topics'] = lda_topic_df['topics'].apply(lambda x: unique_words(x))  # Removing duplicate words\n",
    "    lda_topic_df['topics'] = lda_topic_df['topics'].apply(lambda x: [' '.join(x)])  # Joining each word into a list\n",
    "    lda_topic_df['topics'] = lda_topic_df['topics'].str[0]  # Removing the list brackets\n",
    "\n",
    "    lda_topic_df = lda_topic_df['topics'].reset_index()\n",
    "    lda_topic_df.columns = ['lda_topic_num', 'topics']\n",
    "\n",
    "    # Creating a temp df with the id and topic num to join on\n",
    "    id_ = df['para_id'].tolist()\n",
    "    df_temp = pd.DataFrame({\n",
    "        'para_id': id_,\n",
    "        'lda_topic_num': lda_docweights.argmax(axis=1)\n",
    "    })\n",
    "    merged_topic = df_temp.merge(\n",
    "        lda_topic_df,\n",
    "        on='lda_topic_num',\n",
    "        how='left'\n",
    "    )\n",
    "    # Merging with the original df\n",
    "    df_topics = pd.merge(\n",
    "        df,\n",
    "        merged_topic,\n",
    "        on='para_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # set top label with probabilities\n",
    "    # borrowed from https://stackoverflow.com/questions/35252762/finding-number-of-documents-per-topic-for-lda-with-scikit-learn\n",
    "    df_topics['top_label'] = 'n/a'\n",
    "    docsVStopics = pd.DataFrame(lda_docweights, \n",
    "                                columns=[\"Topic \"+str(i) for i in range(n_top_words)])\n",
    "    most_likely_topics = docsVStopics.idxmax(axis=1)\n",
    "    df_topics['top_label'] = most_likely_topics\n",
    "    label_arr = []\n",
    "    for i in range(len(df_topics)):\n",
    "        idx = int(df_topics.iloc[i][\"top_label\"][-1])\n",
    "        arr = (df_topics.iloc[i][\"topics\"]).split(\" \")\n",
    "        label_arr.append(arr[idx])\n",
    "    df_topics['top_label'] = label_arr\n",
    "\n",
    "    return df_topics\n",
    "\n",
    "def generate_stack_of_papers(len_of_papers = 2, \n",
    "                             num_papers = 5, \n",
    "                             sets = 'random', \n",
    "                             essay_set = 5, \n",
    "                             random_state = 0):\n",
    "    \"\"\" Generates pseudo-papers from the kaggle data set to test logic of grading papers. \"\"\"\n",
    "    from random import seed, randint\n",
    "    # seed random number generator\n",
    "    seed(random_state)\n",
    "    seed(randint(1,25))\n",
    "\n",
    "    stack_of_papers = []\n",
    "    for _ in range(num_papers):\n",
    "        short_paper = []\n",
    "        for i in range(len_of_papers):\n",
    "            short_para = ''\n",
    "            value = randint(1, 8) if sets == 'random' else essay_set\n",
    "            end_range = len(df_topics.loc[df_topics['essay_set'] == value, 'essay'])\n",
    "            value2 = randint(0, end_range)\n",
    "            short_para = df_topics.loc[df_topics['essay_set'] == value, 'essay'].iloc[value2]\n",
    "            short_paper.append(short_para)\n",
    "        stack_of_papers.append(short_paper)\n",
    "\n",
    "    for i in range(len(stack_of_papers)):\n",
    "        stack_of_papers[i] = '\\n'.join(stack_of_papers[i])\n",
    "        \n",
    "    return stack_of_papers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://towardsdatascience.com/an-end-to-end-web-service-implementation-for-text-classification-using-word2vec-and-lgbm-3d605c77fd6e\n",
    "\n",
    "import gensim\n",
    "import datetime\n",
    "\n",
    "VECTOR_SIZE = 150\n",
    "\n",
    "class WordEmbeddingsService:\n",
    "    def __init__(self, raw_text=None, tokenized_text=None):\n",
    "        self.raw_text = raw_text\n",
    "        self.tokenized_text = tokenized_text\n",
    "\n",
    "    def set_raw_text(self, raw_text):\n",
    "        self.raw_text = raw_text\n",
    "\n",
    "    def set_tokenized_text(self, tokenized_text):\n",
    "        self.tokenized_text = tokenized_text\n",
    "\n",
    "    def train_w2v_model(self, tokenized_text):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        model = gensim.models.Word2Vec(tokenized_text,\n",
    "                                       window=50,\n",
    "                                       vector_size=VECTOR_SIZE,\n",
    "                                       epochs=5,\n",
    "                                       min_count=3,\n",
    "                                       workers=1)\n",
    "        return model\n",
    "\n",
    "    def create_word_embeddings(self, text_list, model):\n",
    "        vectors = [self.calculate_avg_vectors(x, model) for x in text_list]\n",
    "        vector_series = pd.DataFrame(vectors).apply(pd.Series).reset_index()\n",
    "        return vector_series\n",
    "\n",
    "    def calculate_avg_vectors(self, text_list, model):\n",
    "        vectors = []\n",
    "        for word in text_list:\n",
    "            vectors.append(self.get_vector(word, model))\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(VECTOR_SIZE)\n",
    "        vec_avg = np.mean(vectors, axis=0)\n",
    "        return vec_avg\n",
    "\n",
    "    def get_vector(self, word, model):\n",
    "        try:\n",
    "            return model.wv.get_vector(word)\n",
    "        except:\n",
    "            return np.zeros(VECTOR_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch workspace\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
