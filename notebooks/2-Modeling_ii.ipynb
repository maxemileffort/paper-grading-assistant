{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Grading Assistant\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Data comes from this link:\n",
    "- https://www.kaggle.com/c/asap-aes/data\n",
    "\n",
    "Heavy inspiration drawn from:\n",
    "- https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "(Use incognito window when opening that link)\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook is the part of the grading process where a teacher might categorize his or her students' papers by letter grade.\n",
    "\n",
    "The idea here is that the teacher will only need to adjust a few grades instead of having to grade an entire stack of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import os, sys\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run the utilty functions from a seperate notebook\n",
    "%run topic_model_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\Kaggle\\\\asap-aes\\\\training_set_rel3.tsv\", sep='\\t')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_essay'] = data.essay.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>tokenized_essay</th>\n",
       "      <th>max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, believe, using, computer, benefit, way,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, people, use, computer, agrees, benefit,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, expert, computer, ben...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, location, know, having, computer, posit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             0.0              8   \n",
       "1               5               4             0.0              9   \n",
       "2               4               3             0.0              7   \n",
       "3               5               5             0.0             10   \n",
       "4               4               4             0.0              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait5  \\\n",
       "0             0.0             0.0            0.0  ...            0.0   \n",
       "1             0.0             0.0            0.0  ...            0.0   \n",
       "2             0.0             0.0            0.0  ...            0.0   \n",
       "3             0.0             0.0            0.0  ...            0.0   \n",
       "4             0.0             0.0            0.0  ...            0.0   \n",
       "\n",
       "   rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   rater3_trait5  rater3_trait6  \\\n",
       "0            0.0            0.0   \n",
       "1            0.0            0.0   \n",
       "2            0.0            0.0   \n",
       "3            0.0            0.0   \n",
       "4            0.0            0.0   \n",
       "\n",
       "                                     tokenized_essay  max_score  \n",
       "0  [dear, local, newspaper, think, effect, comput...          0  \n",
       "1  [dear, believe, using, computer, benefit, way,...          0  \n",
       "2  [dear, people, use, computer, agrees, benefit,...          0  \n",
       "3  [dear, local, newspaper, expert, computer, ben...          0  \n",
       "4  [dear, location, know, having, computer, posit...          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN w/ 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "# add a max_score column to use later \n",
    "# for standardizing scores, as all the \n",
    "# different essays sets have different \n",
    "# scales on which they were scored\n",
    "data['max_score'] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change max score col based on essay set\n",
    "# max vals:\n",
    "# set 1: 12\n",
    "# set 2: 10 or 24, needs some experimenting\n",
    "# set 3: 3\n",
    "# set 4: 3\n",
    "# set 5: 4\n",
    "# set 6: 4\n",
    "# set 7: 30\n",
    "# set 8: 60\n",
    "\n",
    "essay_sets = data.essay_set.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       12\n",
      "1       12\n",
      "2       12\n",
      "3       12\n",
      "4       12\n",
      "        ..\n",
      "1778    12\n",
      "1779    12\n",
      "1780    12\n",
      "1781    12\n",
      "1782    12\n",
      "Name: max_score, Length: 1783, dtype: int64\n",
      "5309    3\n",
      "5310    3\n",
      "5311    3\n",
      "5312    3\n",
      "5313    3\n",
      "       ..\n",
      "7074    3\n",
      "7075    3\n",
      "7076    3\n",
      "7077    3\n",
      "7078    3\n",
      "Name: max_score, Length: 1770, dtype: int64\n",
      "10684    30\n",
      "10685    30\n",
      "10686    30\n",
      "10687    30\n",
      "10688    30\n",
      "         ..\n",
      "12248    30\n",
      "12249    30\n",
      "12250    30\n",
      "12251    30\n",
      "12252    30\n",
      "Name: max_score, Length: 1569, dtype: int64\n",
      "12253    60\n",
      "12254    60\n",
      "12255    60\n",
      "12256    60\n",
      "12257    60\n",
      "         ..\n",
      "12971    60\n",
      "12972    60\n",
      "12973    60\n",
      "12974    60\n",
      "12975    60\n",
      "Name: max_score, Length: 723, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set_ in essay_sets:\n",
    "    if set_ == 1:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 12\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 10\n",
    "    if set_ == 3 or set_ == 4:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 3\n",
    "    if set_ == 5 or set_ == 6:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 4\n",
    "    if set_ == 7:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 30\n",
    "    if set_ == 8:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 60\n",
    "# spot checking some of the data\n",
    "print(data.loc[data.essay_set == 1, 'max_score'])\n",
    "print(data.loc[data.essay_set == 4, 'max_score'])\n",
    "print(data.loc[data.essay_set == 7, 'max_score'])\n",
    "print(data.loc[data.essay_set == 8, 'max_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp column for \n",
    "# model's later internal classes\n",
    "data['temp'] = 0\n",
    "for set_ in essay_sets:\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = (data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   + data.loc[data.essay_set==set_,'domain2_score']) \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']\n",
    "        continue\n",
    "    else:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-cebdbcdee144>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 2\n",
      "<ipython-input-10-cebdbcdee144>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 3\n",
      "<ipython-input-10-cebdbcdee144>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 4\n",
      "<ipython-input-10-cebdbcdee144>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 5\n"
     ]
    }
   ],
   "source": [
    "# re-classify each paper on a scale of 1-5,\n",
    "# with 5 being a high score (like an A on an \n",
    "# ABCDF scale)\n",
    "data['class'] = 1\n",
    "for x in range(len(data)):\n",
    "    if (data.temp[x]) >= .9:\n",
    "        data['class'][x] = 5\n",
    "        continue\n",
    "    elif data.temp[x] >= .8 and data.temp[x] < .9:\n",
    "        data['class'][x] = 4\n",
    "        continue\n",
    "    elif data.temp[x] >= .7 and data.temp[x] < .8:\n",
    "        data['class'][x] = 3\n",
    "        continue\n",
    "    elif data.temp[x] >= .6 and data.temp[x] < .7:\n",
    "        data['class'][x] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# Initialize tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, \n",
    "                                   min_df=3, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english', \n",
    "                                   preprocessor=' '.join)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Bag of words\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, \n",
    "                                min_df=3, \n",
    "                                max_features=no_features, \n",
    "                                stop_words='english', \n",
    "                                preprocessor=' '.join)\n",
    "tf = tf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Word2Vec\n",
    "word2vec = WordEmbeddingsService()\n",
    "word2vec_model = word2vec.train_w2v_model(tokenized_text=data['tokenized_essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a few different vecotrizations of the data\n",
    "# to see which version does the best\n",
    "\n",
    "X_tfidf = tfidf\n",
    "X_tf = tf\n",
    "X_w2v = word2vec.create_word_embeddings(data['tokenized_essay'], word2vec_model)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the different classifiers \n",
    "# to test with the paper scores\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(classifier, X, y, rs=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm, acc_score, prec_score, rec_score = make_confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, f1, prec_score, rec_score\n",
    "\n",
    "def make_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    prec_score = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec_score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, prec_score, rec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all the different classifiers\n",
    "# to loop through.\n",
    "# There are some unsupervised models just for comparison.\n",
    "classifiers = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2),\n",
    "    \"nb\" : MultinomialNB(), \n",
    "    \"log_reg\": LogisticRegression(random_state=0),\n",
    "    \"lin_svm\" : SVC(kernel = 'linear', random_state = 0), # took too long with word2vec (more than 5000 secs)\n",
    "    \"rbf_svm\" : SVC(kernel = 'rbf', random_state = 0),\n",
    "    \"tree\" : DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\n",
    "    \"rf\" : RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\n",
    "    \"ada\" : AdaBoostClassifier(random_state = 0),\n",
    "    \"gb\" : GradientBoostingClassifier(random_state = 0),\n",
    "    \"xgb\" : XGBClassifier(random_state = 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectors first, 3 min\n",
    "tfidf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tfidf, y)\n",
    "    tfidf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }\n",
    "    print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n",
      "[15:16:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with bag of words models, 2.5 min\n",
    "tf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tf, y)\n",
    "    print(\"==============\")\n",
    "    tf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "rbf_svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:20:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with word2vec models, 5 min\n",
    "w2v_res = {}\n",
    "for key in classifiers.keys():\n",
    "    # lin_svm takes more than 1 hour on its own.\n",
    "    # nb doesn't accept negative numbers from the vectors.\n",
    "    if key == 'lin_svm' or key == 'nb': \n",
    "        continue\n",
    "    print(key)\n",
    "    try:\n",
    "        cm, acc, f1, prec, rec = make_classification(classifiers[key], X_w2v, y)\n",
    "    except:\n",
    "        cm, acc, f1, prec, rec = 0,0,0,0,0\n",
    "    print(\"==============\")\n",
    "    w2v_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf acc:  0.4664869029275809\n",
      "tfidf f1:  0.4453934852619973\n",
      "tfidf precision:  0.4465330527617263\n",
      "tfidf recall:  0.4664869029275809\n",
      "==================\n",
      "tf acc:  0.4761171032357473\n",
      "tf f1:  0.4079431179129959\n",
      "tf precision:  0.47850678942735614\n",
      "tf recall:  0.4761171032357473\n",
      "==================\n",
      "w2v acc:  0.38906009244992296\n",
      "w2v f1:  0.3791162483041749\n",
      "w2v precision:  0.37424588959718885\n",
      "w2v recall:  0.38906009244992296\n",
      "==================\n",
      "nb\n",
      "==================\n",
      "tfidf acc:  0.47573189522342063\n",
      "tfidf f1:  0.4631882458365893\n",
      "tfidf precision:  0.49110090372272774\n",
      "tfidf recall:  0.47573189522342063\n",
      "==================\n",
      "tf acc:  0.43567026194144837\n",
      "tf f1:  0.4360088389690266\n",
      "tf precision:  0.4895715592898494\n",
      "tf recall:  0.43567026194144837\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf acc:  0.6055469953775039\n",
      "tfidf f1:  0.5968740446724065\n",
      "tfidf precision:  0.6005266807285446\n",
      "tfidf recall:  0.6055469953775039\n",
      "==================\n",
      "tf acc:  0.598613251155624\n",
      "tf f1:  0.5954524544801241\n",
      "tf precision:  0.5936365476737581\n",
      "tf recall:  0.598613251155624\n",
      "==================\n",
      "w2v acc:  0.47303543913713403\n",
      "w2v f1:  0.45075943931826895\n",
      "w2v precision:  0.45433937231598276\n",
      "w2v recall:  0.47303543913713403\n",
      "==================\n",
      "lin_svm\n",
      "==================\n",
      "tfidf acc:  0.6070878274268104\n",
      "tfidf f1:  0.597898527176032\n",
      "tfidf precision:  0.6118247336528103\n",
      "tfidf recall:  0.6070878274268104\n",
      "==================\n",
      "tf acc:  0.5828197226502311\n",
      "tf f1:  0.5758835560227374\n",
      "tf precision:  0.5725264335105704\n",
      "tf recall:  0.5828197226502311\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf acc:  0.6078582434514638\n",
      "tfidf f1:  0.5960781606395462\n",
      "tfidf precision:  0.6116355773368192\n",
      "tfidf recall:  0.6078582434514638\n",
      "==================\n",
      "tf acc:  0.636748844375963\n",
      "tf f1:  0.6312440029414194\n",
      "tf precision:  0.6438954987730533\n",
      "tf recall:  0.636748844375963\n",
      "==================\n",
      "w2v acc:  0.44414483821263484\n",
      "w2v f1:  0.3403937922016923\n",
      "w2v precision:  0.2766823462591657\n",
      "w2v recall:  0.44414483821263484\n",
      "==================\n",
      "tree\n",
      "==================\n",
      "tfidf acc:  0.48805855161787365\n",
      "tfidf f1:  0.48603772941894896\n",
      "tfidf precision:  0.4843217670752956\n",
      "tfidf recall:  0.48805855161787365\n",
      "==================\n",
      "tf acc:  0.488828967642527\n",
      "tf f1:  0.48446224041277813\n",
      "tf precision:  0.48121434226619236\n",
      "tf recall:  0.488828967642527\n",
      "==================\n",
      "w2v acc:  0.47303543913713403\n",
      "w2v f1:  0.4749952238730022\n",
      "w2v precision:  0.47747157948261976\n",
      "w2v recall:  0.47303543913713403\n",
      "==================\n",
      "rf\n",
      "==================\n",
      "tfidf acc:  0.5573959938366718\n",
      "tfidf f1:  0.5362237600951063\n",
      "tfidf precision:  0.5421790794663678\n",
      "tfidf recall:  0.5573959938366718\n",
      "==================\n",
      "tf acc:  0.5550847457627118\n",
      "tf f1:  0.5313995303218118\n",
      "tf precision:  0.5487227552971509\n",
      "tf recall:  0.5550847457627118\n",
      "==================\n",
      "w2v acc:  0.5350539291217258\n",
      "w2v f1:  0.5280129951483634\n",
      "w2v precision:  0.5251498042945696\n",
      "w2v recall:  0.5350539291217258\n",
      "==================\n",
      "ada\n",
      "==================\n",
      "tfidf acc:  0.4842064714946071\n",
      "tfidf f1:  0.44992061977142916\n",
      "tfidf precision:  0.4642529540106233\n",
      "tfidf recall:  0.4842064714946071\n",
      "==================\n",
      "tf acc:  0.5130970724191063\n",
      "tf f1:  0.5035814795906661\n",
      "tf precision:  0.5030490697619884\n",
      "tf recall:  0.5130970724191063\n",
      "==================\n",
      "w2v acc:  0.4306625577812018\n",
      "w2v f1:  0.38855657349225037\n",
      "w2v precision:  0.40307052447220065\n",
      "w2v recall:  0.4306625577812018\n",
      "==================\n",
      "gb\n",
      "==================\n",
      "tfidf acc:  0.6086286594761171\n",
      "tfidf f1:  0.5972218150682472\n",
      "tfidf precision:  0.6062938961399716\n",
      "tfidf recall:  0.6086286594761171\n",
      "==================\n",
      "tf acc:  0.6013097072419107\n",
      "tf f1:  0.5894196476689069\n",
      "tf precision:  0.5951337547518495\n",
      "tf recall:  0.6013097072419107\n",
      "==================\n",
      "w2v acc:  0.5820493066255779\n",
      "w2v f1:  0.5782495019275442\n",
      "w2v precision:  0.5826019207744982\n",
      "w2v recall:  0.5820493066255779\n",
      "==================\n",
      "xgb\n",
      "==================\n",
      "tfidf acc:  0.6201848998459168\n",
      "tfidf f1:  0.6159521575107308\n",
      "tfidf precision:  0.6166456791967739\n",
      "tfidf recall:  0.6201848998459168\n",
      "==================\n",
      "tf acc:  0.6101694915254238\n",
      "tf f1:  0.604142486456049\n",
      "tf precision:  0.6053164396883034\n",
      "tf recall:  0.6101694915254238\n",
      "==================\n",
      "w2v acc:  0.5728043143297381\n",
      "w2v f1:  0.5720358349682719\n",
      "w2v precision:  0.5725589136823893\n",
      "w2v recall:  0.5728043143297381\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# everything else being equal,\n",
    "# we want the one with highest precisions \n",
    "# (precision is affected by FP, which would be \n",
    "# overestimation of the grade of the paper)\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf acc: \", tfidf_res[key]['acc'])\n",
    "        print(\"tfidf f1: \", tfidf_res[key]['f1'])\n",
    "        print(\"tfidf precision: \", tfidf_res[key]['prec'])\n",
    "        print(\"tfidf recall: \", tfidf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf acc: \", tf_res[key]['acc'])\n",
    "        print(\"tf f1: \", tf_res[key]['f1'])\n",
    "        print(\"tf precision: \", tf_res[key]['prec'])\n",
    "        print(\"tf recall: \", tf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v acc: \", w2v_res[key]['acc'])\n",
    "        print(\"w2v f1: \", w2v_res[key]['f1'])\n",
    "        print(\"w2v precision: \", w2v_res[key]['prec'])\n",
    "        print(\"w2v recall: \", w2v_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the best results from the training above. \n",
    "\n",
    "*Note:* I left out the unsupervised learning models because I generally just like to test them for a \"shot in the dark\" type of look at finding the optimal model. I attribute this to a short stent as a marketer where testing EVERYTHING was an important part of the puzzle.\n",
    "\n",
    "### log_reg\n",
    "- tfidf acc:  0.6171032357473035\n",
    "- tfidf f1:  0.6096852266087265\n",
    "- tfidf precision:  0.6113537557873977\n",
    "- tfidf recall:  0.6171032357473035\n",
    "\n",
    "### lin_svm\n",
    "- tfidf acc:  0.613251155624037\n",
    "- tfidf f1:  0.608159930006525\n",
    "- tfidf precision:  0.6131331303612533\n",
    "- tfidf recall:  0.613251155624037\n",
    "\n",
    "### rbf_svm\n",
    "- tf acc:  0.6475346687211094\n",
    "- tf f1:  0.6415366136770412\n",
    "- tf precision:  0.6535206117830535\n",
    "- tf recall:  0.6475346687211094\n",
    "\n",
    "### tree\n",
    "- tfidf acc:  0.5520030816640986\n",
    "- tfidf f1:  0.5528056512924708\n",
    "- tfidf precision:  0.5539368422055277\n",
    "- tfidf recall:  0.5520030816640986\n",
    "\n",
    "### rf\n",
    "- tfidf acc:  0.6147919876733436\n",
    "- tfidf f1:  0.605882006540668\n",
    "- tfidf precision:  0.6049429246696592\n",
    "- tfidf recall:  0.6147919876733436\n",
    "\n",
    "### ada\n",
    "- tf acc:  0.49768875192604006\n",
    "- tf f1:  0.4845876911067036\n",
    "- tf precision:  0.48788715673893546\n",
    "- tf recall:  0.49768875192604006\n",
    "\n",
    "### gb\n",
    "- tfidf acc:  0.6348228043143297\n",
    "- tfidf f1:  0.6312471077545355\n",
    "- tfidf precision:  0.6348880983202261\n",
    "- tfidf recall:  0.6348228043143297\n",
    "\n",
    "### best word2vec model results (gradient boost)\n",
    "- w2v acc:  0.5963020030816641\n",
    "- w2v f1:  0.594721033739939\n",
    "- w2v precision:  0.5999014604841247\n",
    "- w2v recall:  0.5963020030816641\n",
    "\n",
    "### xgb\n",
    "- tfidf acc:  0.6432973805855162\n",
    "- tfidf f1:  0.6412741241938258\n",
    "- tfidf precision:  0.642001655993716\n",
    "- tfidf recall:  0.6432973805855162\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf cm: \n",
      " [[735 122  98  10  32]\n",
      " [304 208  47  42  36]\n",
      " [204  49 124  39  61]\n",
      " [ 54  53  27  49   5]\n",
      " [ 97  51  39  15  95]]\n",
      "==================\n",
      "tf cm: \n",
      " [[931  45  21   0   0]\n",
      " [472 143  13   7   2]\n",
      " [290  59 104  10  14]\n",
      " [ 76  67  19  25   1]\n",
      " [125  60  72   7  33]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[556 191 154  25  71]\n",
      " [266 225  41  62  43]\n",
      " [186  79 148  22  42]\n",
      " [ 52  73  28  35   0]\n",
      " [122  60  61   8  46]]\n",
      "nb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[590 198 140  53  16]\n",
      " [183 297   0 141  16]\n",
      " [133  69 192  83   0]\n",
      " [ 21  36   0 131   0]\n",
      " [ 70  75  85  42  25]]\n",
      "==================\n",
      "tf cm: \n",
      " [[546 114 115  76 146]\n",
      " [132 159   0 185 161]\n",
      " [104  49 118 102 104]\n",
      " [ 20  23   0 145   0]\n",
      " [ 33  19  39  43 163]]\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf cm: \n",
      " [[795 131  56   6   9]\n",
      " [168 375  18  41  35]\n",
      " [102  95 207  48  25]\n",
      " [ 10  45  25 104   4]\n",
      " [ 18  61  94  33  91]]\n",
      "==================\n",
      "tf cm: \n",
      " [[793 125  60   6  13]\n",
      " [169 324  62  40  42]\n",
      " [ 87  72 219  49  50]\n",
      " [  7  41  47  76  17]\n",
      " [  9  43  72  31 142]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[644 210 104  32   7]\n",
      " [223 288   7  96  23]\n",
      " [155  71 190  61   0]\n",
      " [ 27  56   9  96   0]\n",
      " [ 92  84  95  16  10]]\n",
      "lin_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[761 150  73   5   8]\n",
      " [139 416  14  40  28]\n",
      " [ 91 106 226  45   9]\n",
      " [ 14  52  17 104   1]\n",
      " [ 12  65 110  41  69]]\n",
      "==================\n",
      "tf cm: \n",
      " [[791 126  60   6  14]\n",
      " [207 304  54  30  42]\n",
      " [100  74 216  40  47]\n",
      " [ 10  49  48  68  13]\n",
      " [ 21  45  70  27 134]]\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[771 150  65   4   7]\n",
      " [143 425   5  42  22]\n",
      " [103 108 208  44  14]\n",
      " [ 16  55  10 106   1]\n",
      " [ 17  76  96  40  68]]\n",
      "==================\n",
      "tf cm: \n",
      " [[792 140  56   3   6]\n",
      " [144 427  10  35  21]\n",
      " [ 82 106 215  53  21]\n",
      " [  8  55  13 111   1]\n",
      " [  6  51  89  43 108]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[816 181   0   0   0]\n",
      " [300 337   0   0   0]\n",
      " [340 137   0   0   0]\n",
      " [ 25 163   0   0   0]\n",
      " [212  85   0   0   0]]\n",
      "tree\n",
      "==================\n",
      "tfidf cm: \n",
      " [[665 179  87  21  45]\n",
      " [192 263  71  50  61]\n",
      " [108  76 181  37  75]\n",
      " [ 28  51  47  52  10]\n",
      " [ 40  68  64  19 106]]\n",
      "==================\n",
      "tf cm: \n",
      " [[679 183  89  12  34]\n",
      " [211 248  73  48  57]\n",
      " [123  77 180  36  61]\n",
      " [ 18  45  45  63  17]\n",
      " [ 45  70  67  16  99]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[620 180 115  32  50]\n",
      " [169 270  62  67  69]\n",
      " [108  79 167  51  72]\n",
      " [ 26  42  47  60  13]\n",
      " [ 48  53  72  13 111]]\n",
      "rf\n",
      "==================\n",
      "tfidf cm: \n",
      " [[832 106  44   6   9]\n",
      " [239 299  32  38  29]\n",
      " [154  82 192  32  17]\n",
      " [ 30  69  31  57   1]\n",
      " [ 42  80  88  20  67]]\n",
      "==================\n",
      "tf cm: \n",
      " [[837 116  37   2   5]\n",
      " [284 297  21  24  11]\n",
      " [153 100 182  23  19]\n",
      " [ 26  76  22  61   3]\n",
      " [ 45  75  95  18  64]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[730 155  81   7  24]\n",
      " [185 304  31  52  65]\n",
      " [144  73 182  40  38]\n",
      " [ 14  57  31  79   7]\n",
      " [ 40  57  80  26  94]]\n",
      "ada\n",
      "==================\n",
      "tfidf cm: \n",
      " [[783 142  26  19  27]\n",
      " [273 231  21  59  53]\n",
      " [256  72  73  56  20]\n",
      " [ 23  33  18 100  14]\n",
      " [123  34  37  33  70]]\n",
      "==================\n",
      "tf cm: \n",
      " [[705 163  90  19  20]\n",
      " [195 316  15  63  48]\n",
      " [151  68 160  57  41]\n",
      " [ 23  49  11  84  21]\n",
      " [ 48  74  78  30  67]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[754  78  32  16 117]\n",
      " [333 162  27  59  56]\n",
      " [233  64  32  46 102]\n",
      " [ 24  42  20  90  12]\n",
      " [173  11  12  21  80]]\n",
      "gb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[825 110  46   5  11]\n",
      " [201 362   7  44  23]\n",
      " [109  94 200  48  26]\n",
      " [ 15  62  13  96   2]\n",
      " [ 30  59  72  39  97]]\n",
      "==================\n",
      "tf cm: \n",
      " [[821 113  51   5   7]\n",
      " [215 344  15  34  29]\n",
      " [108  89 201  49  30]\n",
      " [ 14  60  13 100   1]\n",
      " [ 28  58  79  37  95]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[737 156  64  19  21]\n",
      " [148 380  15  44  50]\n",
      " [107  85 195  55  35]\n",
      " [ 12  62  13  96   5]\n",
      " [ 17  61  83  33 103]]\n",
      "xgb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[797 126  62   1  11]\n",
      " [165 376  35  26  35]\n",
      " [ 84  89 224  46  34]\n",
      " [  5  53  31  89  10]\n",
      " [  9  55  70  39 124]]\n",
      "==================\n",
      "tf cm: \n",
      " [[798 136  53   1   9]\n",
      " [180 367  20  29  41]\n",
      " [ 86 101 220  35  35]\n",
      " [  6  64  27  86   5]\n",
      " [ 13  58  85  28 113]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[716 154  94  11  22]\n",
      " [135 358  41  43  60]\n",
      " [107  75 213  40  42]\n",
      " [ 12  45  40  84   7]\n",
      " [ 24  49  79  29 116]]\n"
     ]
    }
   ],
   "source": [
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf cm: \\n\", tfidf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf cm: \\n\", tf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v cm: \\n\", w2v_res[key]['cm'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices just for fun. The best models look to be\n",
    "# SVM with rbf kernel and gradient boosting. Now for some cross validation.\n",
    "\n",
    "# rbf svm uses tf\n",
    "# gb uses tfidif\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "X_tfidf = tfidf\n",
    "X_tf = tf\n",
    "y = data['class']\n",
    "\n",
    "svm_X_train, svm_X_test, svm_y_train, svm_y_test = train_test_split(X_tf, \n",
    "                                                                    y, \n",
    "                                                                    test_size = 0.2, \n",
    "                                                                    random_state = 42)\n",
    "\n",
    "gb_X_train, gb_X_test, gb_y_train, gb_y_test = train_test_split(X_tfidf, \n",
    "                                                                y, \n",
    "                                                                test_size = 0.2, \n",
    "                                                                random_state = 42)\n",
    "\n",
    "svm_accuracies = cross_val_score(estimator = classifiers['rbf_svm'], \n",
    "                                 X = svm_X_train, \n",
    "                                 y = svm_y_train, \n",
    "                                 cv = KFold(shuffle=True))\n",
    "\n",
    "gb_accuracies = cross_val_score(estimator = classifiers['gb'], \n",
    "                                 X = gb_X_train, \n",
    "                                 y = gb_y_train, \n",
    "                                 cv = KFold(shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm Accuracies:  [0.64306358 0.63776493 0.64788054 0.6305395  0.64547206]\n",
      "svm Accuracies mean:  0.6409441233140656\n",
      "GB Accuracies:  [0.59441233 0.60500963 0.60934489 0.61897881 0.60500963]\n",
      "GB Accuracies mean:  0.6065510597302504\n"
     ]
    }
   ],
   "source": [
    "print(\"svm Accuracies: \", svm_accuracies)\n",
    "print(\"svm Accuracies mean: \", svm_accuracies.mean())\n",
    "print(\"GB Accuracies: \", gb_accuracies)\n",
    "print(\"GB Accuracies mean: \", gb_accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both SVM and Gradient boosting have cross-validation accuracies that are in-line with the initial values.\n",
    "\n",
    "SVM has slightly higher accuracies, and better precision, so it's the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(random_state=0), n_jobs=2,\n",
       "             param_grid={'C': [0.01, 1.0, 100.0, 100000.0],\n",
       "                         'gamma': [1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01,\n",
       "                                   'scale', 'auto'],\n",
       "                         'kernel': ('rbf', 'sigmoid')})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have a \"best\" model, it's time to make sure we\n",
    "# are getting the best we can out of it. (takes almost 2 hours)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gamma_range = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,'scale','auto']\n",
    "c_range = [1e-2,1e0,1e2,1e5]\n",
    "svr_param_grid = {\n",
    "    'kernel' : ('rbf', 'sigmoid'),\n",
    "    'C' : c_range,\n",
    "    'gamma' : gamma_range\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(classifiers['rbf_svm'],svr_param_grid,cv=3,n_jobs=2)\n",
    "gs.fit(X_tf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr_cv.best_score_: \n",
      "\n",
      "0.4565330924632794\n",
      "svr_cv.best_params_: \n",
      "\n",
      "{'C': 0.01, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print('svr_cv.best_score_: \\n')\n",
    "print(gs.best_score_)\n",
    "print('svr_cv.best_params_: \\n')\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the default settings are the best! So that'll be the model going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(classifiers['rbf_svm'], 'kaggle_trained_model.joblib') "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e301189debc5177601bb5c59a11b2befed8768132b1b8ef50f2e30593072dd97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
