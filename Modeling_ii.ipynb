{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Grading Assistant\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Data comes from this link:\n",
    "- https://www.kaggle.com/c/asap-aes/data\n",
    "\n",
    "Heavy inspiration drawn from:\n",
    "- https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "(Use incognito window when opening that link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import os, sys\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run the utilty functions from a seperate notebook\n",
    "%run topic_model_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\Kaggle\\\\asap-aes\\\\training_set_rel3.tsv\", sep='\\t')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_essay'] = data.essay.apply(process_text)\n",
    "data['max_score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>tokenized_essay</th>\n",
       "      <th>max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, cap, believe, using, computer, benefit,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, cap, people, use, computer, agrees, ben...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, cap, expert, computer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, location, know, having, computer, posit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             0.0              8   \n",
       "1               5               4             0.0              9   \n",
       "2               4               3             0.0              7   \n",
       "3               5               5             0.0             10   \n",
       "4               4               4             0.0              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait5  \\\n",
       "0             0.0             0.0            0.0  ...            0.0   \n",
       "1             0.0             0.0            0.0  ...            0.0   \n",
       "2             0.0             0.0            0.0  ...            0.0   \n",
       "3             0.0             0.0            0.0  ...            0.0   \n",
       "4             0.0             0.0            0.0  ...            0.0   \n",
       "\n",
       "   rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   rater3_trait5  rater3_trait6  \\\n",
       "0            0.0            0.0   \n",
       "1            0.0            0.0   \n",
       "2            0.0            0.0   \n",
       "3            0.0            0.0   \n",
       "4            0.0            0.0   \n",
       "\n",
       "                                     tokenized_essay  max_score  \n",
       "0  [dear, local, newspaper, think, effect, comput...          0  \n",
       "1  [dear, cap, believe, using, computer, benefit,...          0  \n",
       "2  [dear, cap, people, use, computer, agrees, ben...          0  \n",
       "3  [dear, local, newspaper, cap, expert, computer...          0  \n",
       "4  [dear, location, know, having, computer, posit...          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN w/ 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "# add a max_score column to use later \n",
    "# for standardizing scores, as all the \n",
    "# different essays sets have different \n",
    "# scales on which they were scored\n",
    "data['max_score'] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change max score col based on essay set\n",
    "# max vals:\n",
    "# set 1: 12\n",
    "# set 2: 10 or 24, needs some experimenting\n",
    "# set 3: 3\n",
    "# set 4: 3\n",
    "# set 5: 4\n",
    "# set 6: 4\n",
    "# set 7: 30\n",
    "# set 8: 60\n",
    "\n",
    "essay_sets = data.essay_set.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       12\n",
      "1       12\n",
      "2       12\n",
      "3       12\n",
      "4       12\n",
      "        ..\n",
      "1778    12\n",
      "1779    12\n",
      "1780    12\n",
      "1781    12\n",
      "1782    12\n",
      "Name: max_score, Length: 1783, dtype: int64\n",
      "5309    3\n",
      "5310    3\n",
      "5311    3\n",
      "5312    3\n",
      "5313    3\n",
      "       ..\n",
      "7074    3\n",
      "7075    3\n",
      "7076    3\n",
      "7077    3\n",
      "7078    3\n",
      "Name: max_score, Length: 1770, dtype: int64\n",
      "10684    30\n",
      "10685    30\n",
      "10686    30\n",
      "10687    30\n",
      "10688    30\n",
      "         ..\n",
      "12248    30\n",
      "12249    30\n",
      "12250    30\n",
      "12251    30\n",
      "12252    30\n",
      "Name: max_score, Length: 1569, dtype: int64\n",
      "12253    60\n",
      "12254    60\n",
      "12255    60\n",
      "12256    60\n",
      "12257    60\n",
      "         ..\n",
      "12971    60\n",
      "12972    60\n",
      "12973    60\n",
      "12974    60\n",
      "12975    60\n",
      "Name: max_score, Length: 723, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set_ in essay_sets:\n",
    "    if set_ == 1:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 12\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 10\n",
    "    if set_ == 3 or set_ == 4:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 3\n",
    "    if set_ == 5 or set_ == 6:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 4\n",
    "    if set_ == 7:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 30\n",
    "    if set_ == 8:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 60\n",
    "# spot checking some of the data\n",
    "print(data.loc[data.essay_set == 1, 'max_score'])\n",
    "print(data.loc[data.essay_set == 4, 'max_score'])\n",
    "print(data.loc[data.essay_set == 7, 'max_score'])\n",
    "print(data.loc[data.essay_set == 8, 'max_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp column for \n",
    "# model's later internal classes\n",
    "data['temp'] = 0\n",
    "for set_ in essay_sets:\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = (data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   + data.loc[data.essay_set==set_,'domain2_score']) \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']\n",
    "        continue\n",
    "    else:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-32f58df7bf74>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 2\n",
      "<ipython-input-9-32f58df7bf74>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 3\n",
      "<ipython-input-9-32f58df7bf74>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 4\n",
      "<ipython-input-9-32f58df7bf74>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 5\n"
     ]
    }
   ],
   "source": [
    "# re-classify each paper on a scale of 1-5,\n",
    "# with 5 being a high score (like an A on an \n",
    "# ABCDF scale)\n",
    "data['class'] = 1\n",
    "for x in range(len(data)):\n",
    "    if (data.temp[x]) >= .9:\n",
    "        data['class'][x] = 5\n",
    "        continue\n",
    "    elif data.temp[x] >= .8 and data.temp[x] < .9:\n",
    "        data['class'][x] = 4\n",
    "        continue\n",
    "    elif data.temp[x] >= .7 and data.temp[x] < .8:\n",
    "        data['class'][x] = 3\n",
    "        continue\n",
    "    elif data.temp[x] >= .6 and data.temp[x] < .7:\n",
    "        data['class'][x] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# Initialize tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, \n",
    "                                   min_df=3, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english', \n",
    "                                   preprocessor=' '.join)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Bag of words\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, \n",
    "                                min_df=3, \n",
    "                                max_features=no_features, \n",
    "                                stop_words='english', \n",
    "                                preprocessor=' '.join)\n",
    "tf = tf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Word2Vec\n",
    "word2vec = WordEmbeddingsService()\n",
    "word2vec_model = word2vec.train_w2v_model(tokenized_text=data['tokenized_essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a few different vecotrizations of the data\n",
    "# to see which version does the best\n",
    "\n",
    "X_tfidf = tfidf\n",
    "X_tf = tf\n",
    "X_w2v = word2vec.create_word_embeddings(data['tokenized_essay'], word2vec_model)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the different classifiers \n",
    "# to test with the paper scores\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(classifier, X, y, rs=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm, acc_score, prec_score, rec_score = make_confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, f1, prec_score, rec_score\n",
    "\n",
    "def make_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    prec_score = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec_score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, prec_score, rec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all the different classifiers\n",
    "# to loop through.\n",
    "# There are some unsupervised models just for comparison.\n",
    "classifiers = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2),\n",
    "    \"nb\" : MultinomialNB(), \n",
    "    \"log_reg\": LogisticRegression(random_state=0),\n",
    "    \"lin_svm\" : SVC(kernel = 'linear', random_state = 0), # took too long with word2vec (more than 5000 secs)\n",
    "    \"rbf_svm\" : SVC(kernel = 'rbf', random_state = 0),\n",
    "    \"tree\" : DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\n",
    "    \"rf\" : RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\n",
    "    \"ada\" : AdaBoostClassifier(random_state = 0),\n",
    "    \"gb\" : GradientBoostingClassifier(random_state = 0),\n",
    "    \"xgb\" : XGBClassifier(random_state = 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:00:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectors first, 3 min\n",
    "tfidf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tfidf, y)\n",
    "    tfidf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }\n",
    "    print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n",
      "[14:03:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with bag of words models, 2.5 min\n",
    "tf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tf, y)\n",
    "    print(\"==============\")\n",
    "    tf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "rbf_svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:07:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with word2vec models, 5 min\n",
    "w2v_res = {}\n",
    "for key in classifiers.keys():\n",
    "    # lin_svm takes more than 1 hour on its own.\n",
    "    # nb doesn't accept negative numbers from the vectors.\n",
    "    if key == 'lin_svm' or key == 'nb': \n",
    "        continue\n",
    "    print(key)\n",
    "    try:\n",
    "        cm, acc, f1, prec, rec = make_classification(classifiers[key], X_w2v, y)\n",
    "    except:\n",
    "        cm, acc, f1, prec, rec = 0,0,0,0,0\n",
    "    print(\"==============\")\n",
    "    w2v_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf acc:  0.45377503852080125\n",
      "tfidf f1:  0.38660270480340003\n",
      "tfidf precision:  0.45451067082238295\n",
      "tfidf recall:  0.45377503852080125\n",
      "==================\n",
      "tf acc:  0.41640986132511554\n",
      "tf f1:  0.2857625663744916\n",
      "tf precision:  0.45449679052202635\n",
      "tf recall:  0.41640986132511554\n",
      "==================\n",
      "w2v acc:  0.3983050847457627\n",
      "w2v f1:  0.3853732858118746\n",
      "w2v precision:  0.3804922404417706\n",
      "w2v recall:  0.3983050847457627\n",
      "==================\n",
      "nb\n",
      "==================\n",
      "tfidf acc:  0.487673343605547\n",
      "tfidf f1:  0.4733505994306289\n",
      "tfidf precision:  0.5085628883422912\n",
      "tfidf recall:  0.487673343605547\n",
      "==================\n",
      "tf acc:  0.4464560862865948\n",
      "tf f1:  0.450944452802403\n",
      "tf precision:  0.48771433931539104\n",
      "tf recall:  0.4464560862865948\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf acc:  0.6167180277349769\n",
      "tfidf f1:  0.6092357866617898\n",
      "tfidf precision:  0.6108999346788971\n",
      "tfidf recall:  0.6167180277349769\n",
      "==================\n",
      "tf acc:  0.5982280431432974\n",
      "tf f1:  0.5946366613761039\n",
      "tf precision:  0.5926540993034797\n",
      "tf recall:  0.5982280431432974\n",
      "==================\n",
      "w2v acc:  0.4530046224961479\n",
      "w2v f1:  0.42431693929298675\n",
      "w2v precision:  0.4109848284751718\n",
      "w2v recall:  0.4530046224961479\n",
      "==================\n",
      "lin_svm\n",
      "==================\n",
      "tfidf acc:  0.615562403697997\n",
      "tfidf f1:  0.6104766001381381\n",
      "tfidf precision:  0.615310243436091\n",
      "tfidf recall:  0.615562403697997\n",
      "==================\n",
      "tf acc:  0.588597842835131\n",
      "tf f1:  0.583016126744233\n",
      "tf precision:  0.5809007041223846\n",
      "tf recall:  0.588597842835131\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf acc:  0.6294298921417566\n",
      "tfidf f1:  0.6213631639429569\n",
      "tfidf precision:  0.6338712067950151\n",
      "tfidf recall:  0.6294298921417566\n",
      "==================\n",
      "tf acc:  0.649075500770416\n",
      "tf f1:  0.6436681239615613\n",
      "tf precision:  0.6563417588676188\n",
      "tf recall:  0.649075500770416\n",
      "==================\n",
      "w2v acc:  0.44414483821263484\n",
      "w2v f1:  0.340410751866202\n",
      "w2v precision:  0.27669044799694253\n",
      "w2v recall:  0.44414483821263484\n",
      "==================\n",
      "tree\n",
      "==================\n",
      "tfidf acc:  0.5481510015408321\n",
      "tfidf f1:  0.5475146607660932\n",
      "tfidf precision:  0.5477116157728208\n",
      "tfidf recall:  0.5481510015408321\n",
      "==================\n",
      "tf acc:  0.5092449922958397\n",
      "tf f1:  0.506214522695605\n",
      "tf precision:  0.5041357933422754\n",
      "tf recall:  0.5092449922958397\n",
      "==================\n",
      "w2v acc:  0.4703389830508475\n",
      "w2v f1:  0.4747871919789561\n",
      "w2v precision:  0.4811830897620753\n",
      "w2v recall:  0.4703389830508475\n",
      "==================\n",
      "rf\n",
      "==================\n",
      "tfidf acc:  0.6105546995377504\n",
      "tfidf f1:  0.6028896921376169\n",
      "tfidf precision:  0.6029597553402852\n",
      "tfidf recall:  0.6105546995377504\n",
      "==================\n",
      "tf acc:  0.5419876733436055\n",
      "tf f1:  0.5157050963035266\n",
      "tf precision:  0.530067147772547\n",
      "tf recall:  0.5419876733436055\n",
      "==================\n",
      "w2v acc:  0.5620184899845917\n",
      "w2v f1:  0.5588789902149259\n",
      "w2v precision:  0.5572362486309888\n",
      "w2v recall:  0.5620184899845917\n",
      "==================\n",
      "ada\n",
      "==================\n",
      "tfidf acc:  0.4953775038520801\n",
      "tfidf f1:  0.49004674748106525\n",
      "tfidf precision:  0.49729739643831844\n",
      "tfidf recall:  0.4953775038520801\n",
      "==================\n",
      "tf acc:  0.5165639445300462\n",
      "tf f1:  0.507977985971307\n",
      "tf precision:  0.5094132023967732\n",
      "tf recall:  0.5165639445300462\n",
      "==================\n",
      "w2v acc:  0.43875192604006163\n",
      "w2v f1:  0.4024160187961838\n",
      "w2v precision:  0.42169626866636134\n",
      "w2v recall:  0.43875192604006163\n",
      "==================\n",
      "gb\n",
      "==================\n",
      "tfidf acc:  0.6417565485362096\n",
      "tfidf f1:  0.6384250107607738\n",
      "tfidf precision:  0.6419947325259667\n",
      "tfidf recall:  0.6417565485362096\n",
      "==================\n",
      "tf acc:  0.6028505392912172\n",
      "tf f1:  0.5901232090830786\n",
      "tf precision:  0.6006181263684135\n",
      "tf recall:  0.6028505392912172\n",
      "==================\n",
      "w2v acc:  0.5947611710323575\n",
      "w2v f1:  0.5937415882126519\n",
      "w2v precision:  0.5975941866042422\n",
      "w2v recall:  0.5947611710323575\n",
      "==================\n",
      "xgb\n",
      "==================\n",
      "tfidf acc:  0.6398305084745762\n",
      "tfidf f1:  0.637676742412004\n",
      "tfidf precision:  0.6384905398049895\n",
      "tfidf recall:  0.6398305084745762\n",
      "==================\n",
      "tf acc:  0.6005392912172574\n",
      "tf f1:  0.5942131829034566\n",
      "tf precision:  0.5970661979035242\n",
      "tf recall:  0.6005392912172574\n",
      "==================\n",
      "w2v acc:  0.5878274268104776\n",
      "w2v f1:  0.5887086801201667\n",
      "w2v precision:  0.5908229751951412\n",
      "w2v recall:  0.5878274268104776\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# everything else being equal,\n",
    "# we want the one with highest precisions \n",
    "# (precision is affected by FP, which would be \n",
    "# overestimation of the grade of the paper)\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf acc: \", tfidf_res[key]['acc'])\n",
    "        print(\"tfidf f1: \", tfidf_res[key]['f1'])\n",
    "        print(\"tfidf precision: \", tfidf_res[key]['prec'])\n",
    "        print(\"tfidf recall: \", tfidf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf acc: \", tf_res[key]['acc'])\n",
    "        print(\"tf f1: \", tf_res[key]['f1'])\n",
    "        print(\"tf precision: \", tf_res[key]['prec'])\n",
    "        print(\"tf recall: \", tf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v acc: \", w2v_res[key]['acc'])\n",
    "        print(\"w2v f1: \", w2v_res[key]['f1'])\n",
    "        print(\"w2v precision: \", w2v_res[key]['prec'])\n",
    "        print(\"w2v recall: \", w2v_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the best results from the training above. \n",
    "\n",
    "*Note:* I left out the unsupervised learning models because I generally just like to test them for a \"shot in the dark\" type of look at finding the optimal model. I attribute this to a short stent as a marketer where testing EVERYTHING was an important part of the puzzle.\n",
    "\n",
    "### log_reg\n",
    "- tfidf acc:  0.6879815100154083\n",
    "- tfidf f1:  0.6826731058258083\n",
    "- tfidf precision:  0.6840742515662983\n",
    "- tfidf recall:  0.6879815100154083\n",
    "\n",
    "### lin_svm\n",
    "- tfidf acc:  0.6798921417565486\n",
    "- tfidf f1:  0.6796612464208107\n",
    "- tfidf precision:  0.6851511607249138\n",
    "- tfidf recall:  0.6798921417565486\n",
    "\n",
    "### rbf_svm\n",
    "- tf acc:  0.714175654853621\n",
    "- tf f1:  0.7137245090277426\n",
    "- tf precision:  0.7245950249260565\n",
    "- tf recall:  0.714175654853621\n",
    "\n",
    "### tree\n",
    "- tfidf acc:  0.6302003081664098\n",
    "- tfidf f1:  0.6289259312408813\n",
    "- tfidf precision:  0.6278195501435767\n",
    "- tfidf recall:  0.6302003081664098\n",
    "\n",
    "### rf\n",
    "- tfidf acc:  0.687211093990755\n",
    "- tfidf f1:  0.6795152273421196\n",
    "- tfidf precision:  0.6789466187123883\n",
    "- tfidf recall:  0.687211093990755\n",
    "\n",
    "### ada\n",
    "- tf acc:  0.5670261941448382\n",
    "- tf f1:  0.5388070054332165\n",
    "- tf precision:  0.5626726131804644\n",
    "- tf recall:  0.5670261941448382\n",
    "\n",
    "\n",
    "### gb\n",
    "- tfidf acc:  0.7126348228043143\n",
    "- tfidf f1:  0.7112178825280995\n",
    "- tfidf precision:  0.7124145728645078\n",
    "- tfidf recall:  0.7126348228043143\n",
    "\n",
    "### These were the best of the word2vec models (Gradient boosting)\n",
    "- w2v acc:  0.6771956856702619\n",
    "- w2v f1:  0.6786444062419466\n",
    "- w2v precision:  0.6813704223813889\n",
    "- w2v recall:  0.6771956856702619\n",
    "\n",
    "### xgb\n",
    "- tfidf acc:  0.7245762711864406\n",
    "- tfidf f1:  0.7223852160700287\n",
    "- tfidf precision:  0.7212926330136676\n",
    "- tfidf recall:  0.7245762711864406\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf cm: \n",
      " [[900  63  31   0   3]\n",
      " [496  99  11  18  13]\n",
      " [296  27  97  19  38]\n",
      " [132  17   7  32   0]\n",
      " [162  27  48  10  50]]\n",
      "==================\n",
      "tf cm: \n",
      " [[988   7   2   0   0]\n",
      " [609  26   1   0   1]\n",
      " [404  11  56   0   6]\n",
      " [154  28   5   1   0]\n",
      " [210  22  55   0  10]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[583 189 139  25  61]\n",
      " [270 228  41  57  41]\n",
      " [202  69 146  28  32]\n",
      " [ 47  69  37  34   1]\n",
      " [126  65  57   6  43]]\n",
      "nb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[599 204 136  48  10]\n",
      " [181 307   0 136  13]\n",
      " [125  70 197  85   0]\n",
      " [ 20  29   0 139   0]\n",
      " [ 69  72  88  44  24]]\n",
      "==================\n",
      "tf cm: \n",
      " [[530 168 147  88  64]\n",
      " [137 192   0 219  89]\n",
      " [107  45 198 115  12]\n",
      " [ 16  14   1 157   0]\n",
      " [ 43  35  90  47  82]]\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf cm: \n",
      " [[814 118  53   3   9]\n",
      " [169 367  22  41  38]\n",
      " [ 97  94 219  41  26]\n",
      " [  7  53  25  97   6]\n",
      " [ 14  54  91  34 104]]\n",
      "==================\n",
      "tf cm: \n",
      " [[800 113  63   9  12]\n",
      " [164 315  59  47  52]\n",
      " [ 90  71 216  48  52]\n",
      " [  7  39  35  84  23]\n",
      " [  8  54  73  24 138]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[646 214  89  37  11]\n",
      " [273 273   3  88   0]\n",
      " [164  77 181  52   3]\n",
      " [ 27  60  26  75   0]\n",
      " [139  53  91  13   1]]\n",
      "lin_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[789 134  64   1   9]\n",
      " [142 394  24  43  34]\n",
      " [ 83 112 213  39  30]\n",
      " [  9  55  23  99   2]\n",
      " [ 10  53  92  39 103]]\n",
      "==================\n",
      "tf cm: \n",
      " [[785 120  76   5  11]\n",
      " [191 325  53  31  37]\n",
      " [ 96  84 215  36  46]\n",
      " [  9  48  47  75   9]\n",
      " [ 24  41  79  25 128]]\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[797 134  58   1   7]\n",
      " [143 426  11  35  22]\n",
      " [ 88 115 212  42  20]\n",
      " [ 12  59  11 105   1]\n",
      " [ 10  65  86  42  94]]\n",
      "==================\n",
      "tf cm: \n",
      " [[809 131  51   0   6]\n",
      " [138 430  11  34  24]\n",
      " [ 70 119 223  44  21]\n",
      " [  4  57  12 115   0]\n",
      " [  5  53  84  47 108]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[816 181   0   0   0]\n",
      " [300 337   0   0   0]\n",
      " [340 137   0   0   0]\n",
      " [ 25 163   0   0   0]\n",
      " [211  86   0   0   0]]\n",
      "tree\n",
      "==================\n",
      "tfidf cm: \n",
      " [[721 152  81  18  25]\n",
      " [171 291  79  44  52]\n",
      " [ 97  68 189  56  67]\n",
      " [ 11  34  42  78  23]\n",
      " [ 16  51  61  25 144]]\n",
      "==================\n",
      "tf cm: \n",
      " [[691 163  86  18  39]\n",
      " [193 270  65  52  57]\n",
      " [121  53 187  46  70]\n",
      " [ 19  55  38  58  18]\n",
      " [ 38  64  61  18 116]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[605 190 137  17  48]\n",
      " [166 241  97  67  66]\n",
      " [106  67 190  50  64]\n",
      " [ 17  54  36  62  19]\n",
      " [ 22  49  80  23 123]]\n",
      "rf\n",
      "==================\n",
      "tfidf cm: \n",
      " [[818 110  52   7  10]\n",
      " [207 332  38  28  32]\n",
      " [105  87 213  40  32]\n",
      " [  7  61  27  87   6]\n",
      " [ 17  50  69  26 135]]\n",
      "==================\n",
      "tf cm: \n",
      " [[835 111  42   5   4]\n",
      " [281 289  22  32  13]\n",
      " [168  96 163  27  23]\n",
      " [ 25  71  26  65   1]\n",
      " [ 45  89  85  23  55]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[734 148  78  10  27]\n",
      " [173 327  51  33  53]\n",
      " [106  75 212  46  38]\n",
      " [ 14  52  44  70   8]\n",
      " [ 23  53  77  28 116]]\n",
      "ada\n",
      "==================\n",
      "tfidf cm: \n",
      " [[680  99 127   9  82]\n",
      " [242 222  39  57  77]\n",
      " [111  63 161  54  88]\n",
      " [ 12  32  24 101  19]\n",
      " [ 67  20  57  31 122]]\n",
      "==================\n",
      "tf cm: \n",
      " [[693 183  74  19  28]\n",
      " [193 309  12  63  60]\n",
      " [170  72 152  60  23]\n",
      " [ 17  38  15 103  15]\n",
      " [ 36  60  88  29  84]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[767  63  54  16  97]\n",
      " [385 169  18  56   9]\n",
      " [191  71  72  55  88]\n",
      " [ 20  61   8  81  18]\n",
      " [178   8  30  31  50]]\n",
      "gb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[803 126  56   1  11]\n",
      " [155 388  20  40  34]\n",
      " [ 78  90 215  56  38]\n",
      " [  8  50   9 110  11]\n",
      " [ 10  40  63  34 150]]\n",
      "==================\n",
      "tf cm: \n",
      " [[819 116  52   4   6]\n",
      " [211 354  13  38  21]\n",
      " [112  95 203  45  22]\n",
      " [ 16  57  12 101   2]\n",
      " [ 31  67  76  35  88]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[724 155  77  10  31]\n",
      " [132 391  17  48  49]\n",
      " [ 93  88 209  54  33]\n",
      " [ 15  51  15  99   8]\n",
      " [ 18  42  82  34 121]]\n",
      "xgb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[801 132  50   1  13]\n",
      " [151 384  40  31  31]\n",
      " [ 74  84 228  48  43]\n",
      " [  3  48  30  94  13]\n",
      " [  6  47  57  33 154]]\n",
      "==================\n",
      "tf cm: \n",
      " [[795 133  56   2  11]\n",
      " [173 360  40  34  30]\n",
      " [ 88 104 211  42  32]\n",
      " [  9  59  26  91   3]\n",
      " [ 10  62  87  36 102]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[735 149  80  12  21]\n",
      " [140 349  52  42  54]\n",
      " [ 84  78 219  54  42]\n",
      " [  5  41  42  91   9]\n",
      " [ 15  45  78  27 132]]\n"
     ]
    }
   ],
   "source": [
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf cm: \\n\", tfidf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf cm: \\n\", tf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v cm: \\n\", w2v_res[key]['cm'])\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e301189debc5177601bb5c59a11b2befed8768132b1b8ef50f2e30593072dd97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
