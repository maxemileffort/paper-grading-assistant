{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Paper Grading Assistant\r\n",
    "\r\n",
    "## Modeling\r\n",
    "\r\n",
    "Data comes from this link:\r\n",
    "- https://www.kaggle.com/c/asap-aes/data\r\n",
    "\r\n",
    "Heavy inspiration drawn from:\r\n",
    "- https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\r\n",
    "\r\n",
    "(Use incognito window when opening that link)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !pip install gensim\r\n",
    "import os, sys\r\n",
    "from gensim import corpora, models\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import re\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Run the utilty functions from a seperate notebook\r\n",
    "%run topic_model_utils.ipynb"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv(\"D:\\\\Kaggle\\\\asap-aes\\\\training_set_rel3.tsv\", sep='\\t')\r\n",
    "# data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data['tokenized_essay'] = data.essay.apply(process_text)\r\n",
    "data['max_score'] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# replace NaN w/ 0\r\n",
    "data = data.fillna(0)\r\n",
    "\r\n",
    "# add a max_score column to use later \r\n",
    "# for standardizing scores\r\n",
    "data['max_score'] = 0\r\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>tokenized_essay</th>\n",
       "      <th>max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, cap, believe, using, computer, benefit,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, cap, people, use, computer, agrees, ben...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, cap, expert, computer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, location, know, having, computer, posit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             0.0              8   \n",
       "1               5               4             0.0              9   \n",
       "2               4               3             0.0              7   \n",
       "3               5               5             0.0             10   \n",
       "4               4               4             0.0              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait5  \\\n",
       "0             0.0             0.0            0.0  ...            0.0   \n",
       "1             0.0             0.0            0.0  ...            0.0   \n",
       "2             0.0             0.0            0.0  ...            0.0   \n",
       "3             0.0             0.0            0.0  ...            0.0   \n",
       "4             0.0             0.0            0.0  ...            0.0   \n",
       "\n",
       "   rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   rater3_trait5  rater3_trait6  \\\n",
       "0            0.0            0.0   \n",
       "1            0.0            0.0   \n",
       "2            0.0            0.0   \n",
       "3            0.0            0.0   \n",
       "4            0.0            0.0   \n",
       "\n",
       "                                     tokenized_essay  max_score  \n",
       "0  [dear, local, newspaper, think, effect, comput...          0  \n",
       "1  [dear, cap, believe, using, computer, benefit,...          0  \n",
       "2  [dear, cap, people, use, computer, agrees, ben...          0  \n",
       "3  [dear, local, newspaper, cap, expert, computer...          0  \n",
       "4  [dear, location, know, having, computer, posit...          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# change max score col based on essay set\r\n",
    "# max vals:\r\n",
    "# set 1: 12\r\n",
    "# set 2: 10 or 24, needs some experimenting\r\n",
    "# set 3: 3\r\n",
    "# set 4: 3\r\n",
    "# set 5: 4\r\n",
    "# set 6: 4\r\n",
    "# set 7: 30\r\n",
    "# set 8: 60\r\n",
    "\r\n",
    "essay_sets = data.essay_set.unique()\r\n",
    "for set_ in essay_sets:\r\n",
    "    print(set_)\r\n",
    "    if set_ == 1:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 12\r\n",
    "    if set_ == 2:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 10\r\n",
    "    if set_ == 3 or set_ == 4:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 3\r\n",
    "    if set_ == 5 or set_ == 6:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 4\r\n",
    "    if set_ == 7:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 30\r\n",
    "    if set_ == 8:\r\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 60\r\n",
    "print(data.loc[data.essay_set == 1, 'max_score'])\r\n",
    "print(data.loc[data.essay_set == 4, 'max_score'])\r\n",
    "print(data.loc[data.essay_set == 7, 'max_score'])\r\n",
    "print(data.loc[data.essay_set == 8, 'max_score'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0       12\n",
      "1       12\n",
      "2       12\n",
      "3       12\n",
      "4       12\n",
      "        ..\n",
      "1778    12\n",
      "1779    12\n",
      "1780    12\n",
      "1781    12\n",
      "1782    12\n",
      "Name: max_score, Length: 1783, dtype: int64\n",
      "5309    3\n",
      "5310    3\n",
      "5311    3\n",
      "5312    3\n",
      "5313    3\n",
      "       ..\n",
      "7074    3\n",
      "7075    3\n",
      "7076    3\n",
      "7077    3\n",
      "7078    3\n",
      "Name: max_score, Length: 1770, dtype: int64\n",
      "10684    30\n",
      "10685    30\n",
      "10686    30\n",
      "10687    30\n",
      "10688    30\n",
      "         ..\n",
      "12248    30\n",
      "12249    30\n",
      "12250    30\n",
      "12251    30\n",
      "12252    30\n",
      "Name: max_score, Length: 1569, dtype: int64\n",
      "12253    60\n",
      "12254    60\n",
      "12255    60\n",
      "12256    60\n",
      "12257    60\n",
      "         ..\n",
      "12971    60\n",
      "12972    60\n",
      "12973    60\n",
      "12974    60\n",
      "12975    60\n",
      "Name: max_score, Length: 723, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# create temp column for \r\n",
    "# models later internal classes\r\n",
    "data['temp'] = 0\r\n",
    "for set_ in essay_sets:\r\n",
    "    if set_ == 2:\r\n",
    "        data.loc[data.essay_set == set_, 'temp'] = data.loc[data.essay_set==set_,'domain1_score'] \\\r\n",
    "                                                   + data.loc[data.essay_set==set_,'domain2_score'] \\\r\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        data.loc[data.essay_set == set_, 'temp'] = data.loc[data.essay_set==set_,'domain1_score'] \\\r\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# re-classify each paper on a scale of 1-5,\r\n",
    "# with 5 being a high score (like an A on an \r\n",
    "# ABCDF scale)\r\n",
    "data['class'] = 1\r\n",
    "for x in range(len(data)):\r\n",
    "    if (data.temp[x]) >= .9:\r\n",
    "        data['class'][x] = 5\r\n",
    "        continue\r\n",
    "    elif data.temp[x] >= .8 and data.temp[x] < .9:\r\n",
    "        data['class'][x] = 4\r\n",
    "        continue\r\n",
    "    elif data.temp[x] >= .7 and data.temp[x] < .8:\r\n",
    "        data['class'][x] = 3\r\n",
    "        continue\r\n",
    "    elif data.temp[x] >= .6 and data.temp[x] < .7:\r\n",
    "        data['class'][x] = 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-8-0a8b1fd4f84f>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 2\n",
      "<ipython-input-8-0a8b1fd4f84f>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 3\n",
      "<ipython-input-8-0a8b1fd4f84f>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 4\n",
      "<ipython-input-8-0a8b1fd4f84f>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
    "\r\n",
    "no_features = 1000\r\n",
    "\r\n",
    "# Initialize tf-idf\r\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, \r\n",
    "                                   min_df=3, \r\n",
    "                                   max_features=no_features, \r\n",
    "                                   stop_words='english', \r\n",
    "                                   preprocessor=' '.join)\r\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tokenized_essay'])\r\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\r\n",
    "\r\n",
    "# Bag of words\r\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, \r\n",
    "                                min_df=3, \r\n",
    "                                max_features=no_features, \r\n",
    "                                stop_words='english', \r\n",
    "                                preprocessor=' '.join)\r\n",
    "tf = tf_vectorizer.fit_transform(data['tokenized_essay'])\r\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\r\n",
    "\r\n",
    "# Word2Vec\r\n",
    "word2vec = WordEmbeddingsService()\r\n",
    "word2vec_model = word2vec.train_w2v_model(tokenized_text=data['tokenized_essay'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# create a few different vecotrizations of the data\r\n",
    "# to see which version does the best\r\n",
    "\r\n",
    "X_tfidf = tfidf\r\n",
    "X_tf = tf\r\n",
    "X_w2v = word2vec.create_word_embeddings(data['tokenized_essay'], word2vec_model)\r\n",
    "y = data['class']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# import all the different classifiers \r\n",
    "# to test with the paper scores\r\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "import warnings\r\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\r\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\r\n",
    "\r\n",
    "from xgboost import XGBClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def make_classification(classifier, X, y, rs=42):\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rs)\r\n",
    "    # try:\r\n",
    "    #     classifier.fit(X_train, y_train)\r\n",
    "    # except:\r\n",
    "    #     X_train = np.array(X_train)\r\n",
    "    #     X_test = np.array(X_test)\r\n",
    "    #     y_train = np.array(y_train)\r\n",
    "    #     y_test = np.array(y_test)\r\n",
    "    #     classifier.fit(X_train, y_train)\r\n",
    "    classifier.fit(X_train, y_train)\r\n",
    "    y_pred = classifier.predict(X_test)\r\n",
    "    cm, acc_score, prec_score, rec_score = make_confusion_matrix(y_test, y_pred)\r\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\r\n",
    "    return cm, acc_score, f1, prec_score, rec_score\r\n",
    "\r\n",
    "def make_confusion_matrix(y_test, y_pred):\r\n",
    "    cm = confusion_matrix(y_test, y_pred)\r\n",
    "    acc_score = accuracy_score(y_test, y_pred)\r\n",
    "    prec_score = precision_score(y_test, y_pred, average='weighted')\r\n",
    "    rec_score = recall_score(y_test, y_pred, average='weighted')\r\n",
    "    return cm, acc_score, prec_score, rec_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# create a dictionary of all the different classifiers\r\n",
    "# to loop through.\r\n",
    "# There are some models for unsupervised just for comparison.\r\n",
    "classifiers = {\r\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2),\r\n",
    "    \"nb\" : MultinomialNB(), \r\n",
    "    \"log_reg\": LogisticRegression(random_state=0),\r\n",
    "    \"lin_svm\" : SVC(kernel = 'linear', random_state = 0), # took too long with word2vec (more than 5000 secs)\r\n",
    "    \"rbf_svm\" : SVC(kernel = 'rbf', random_state = 0),\r\n",
    "    \"tree\" : DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\r\n",
    "    \"rf\" : RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\r\n",
    "    \"ada\" : AdaBoostClassifier(random_state = 0),\r\n",
    "    \"gb\" : GradientBoostingClassifier(random_state = 0),\r\n",
    "    \"xgb\" : XGBClassifier(random_state = 0),\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# tfidf vectors first, 3 min\r\n",
    "tfidf_res = {}\r\n",
    "for key in classifiers.keys():\r\n",
    "    print(key)\r\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tfidf, y)\r\n",
    "    tfidf_res[key] = {\r\n",
    "        'cm' : cm,\r\n",
    "        'acc' : acc,\r\n",
    "        'f1' : f1,\r\n",
    "        'prec' : prec,\r\n",
    "        'rec' : rec\r\n",
    "    }\r\n",
    "    print(\"==============\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15:45:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# repeat classification with bag of words models, 2.5 min\r\n",
    "tf_res = {}\r\n",
    "for key in classifiers.keys():\r\n",
    "    print(key)\r\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tf, y)\r\n",
    "    print(\"==============\")\r\n",
    "    tf_res[key] = {\r\n",
    "        'cm' : cm,\r\n",
    "        'acc' : acc,\r\n",
    "        'f1' : f1,\r\n",
    "        'prec' : prec,\r\n",
    "        'rec' : rec\r\n",
    "    }"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n",
      "[15:48:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# repeat classification with word2vec models, 5 min\r\n",
    "w2v_res = {}\r\n",
    "for key in classifiers.keys():\r\n",
    "    # lin_svm takes more than 1 hour on its own.\r\n",
    "    # nb doesn't accept negative numbers from the vectors.\r\n",
    "    if key == 'lin_svm' or key == 'nb': \r\n",
    "        continue\r\n",
    "    print(key)\r\n",
    "    try:\r\n",
    "        cm, acc, f1, prec, rec = make_classification(classifiers[key], X_w2v, y)\r\n",
    "    except:\r\n",
    "        cm, acc, f1, prec, rec = 0,0,0,0,0\r\n",
    "    print(\"==============\")\r\n",
    "    w2v_res[key] = {\r\n",
    "        'cm' : cm,\r\n",
    "        'acc' : acc,\r\n",
    "        'f1' : f1,\r\n",
    "        'prec' : prec,\r\n",
    "        'rec' : rec\r\n",
    "    }"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n",
      "rbf_svm\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15:55:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# everything else being equal,\r\n",
    "# we want the one with highest precisions \r\n",
    "# (precision is affected by FP, which would be \r\n",
    "# overestimation of the grade of the paper)\r\n",
    "\r\n",
    "for key in classifiers.keys():\r\n",
    "    try:\r\n",
    "        print(key)\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"tfidf acc: \", tfidf_res[key]['acc'])\r\n",
    "        print(\"tfidf f1: \", tfidf_res[key]['f1'])\r\n",
    "        print(\"tfidf precision: \", tfidf_res[key]['prec'])\r\n",
    "        print(\"tfidf recall: \", tfidf_res[key]['rec'])\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"tf acc: \", tf_res[key]['acc'])\r\n",
    "        print(\"tf f1: \", tf_res[key]['f1'])\r\n",
    "        print(\"tf precision: \", tf_res[key]['prec'])\r\n",
    "        print(\"tf recall: \", tf_res[key]['rec'])\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"w2v acc: \", w2v_res[key]['acc'])\r\n",
    "        print(\"w2v f1: \", w2v_res[key]['f1'])\r\n",
    "        print(\"w2v precision: \", w2v_res[key]['prec'])\r\n",
    "        print(\"w2v recall: \", w2v_res[key]['rec'])\r\n",
    "        print(\"==================\")\r\n",
    "    except:\r\n",
    "        pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf acc:  0.5597072419106317\n",
      "tfidf f1:  0.5180089403445932\n",
      "tfidf precision:  0.5780338452801514\n",
      "tfidf recall:  0.5597072419106317\n",
      "==================\n",
      "tf acc:  0.5050077041602465\n",
      "tf f1:  0.42539275142673827\n",
      "tf precision:  0.5511446307730005\n",
      "tf recall:  0.5050077041602465\n",
      "==================\n",
      "w2v acc:  0.5057781201848999\n",
      "w2v f1:  0.5024756948003423\n",
      "w2v precision:  0.5067483884578161\n",
      "w2v recall:  0.5057781201848999\n",
      "==================\n",
      "nb\n",
      "==================\n",
      "tfidf acc:  0.573959938366718\n",
      "tfidf f1:  0.589093281704411\n",
      "tfidf precision:  0.6358654965846269\n",
      "tfidf recall:  0.573959938366718\n",
      "==================\n",
      "tf acc:  0.5450693374422187\n",
      "tf f1:  0.5756543060296143\n",
      "tf precision:  0.655096792659111\n",
      "tf recall:  0.5450693374422187\n",
      "==================\n",
      "w2v acc:  0\n",
      "w2v f1:  0\n",
      "w2v precision:  0\n",
      "w2v recall:  0\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf acc:  0.6879815100154083\n",
      "tfidf f1:  0.6826731058258083\n",
      "tfidf precision:  0.6840742515662983\n",
      "tfidf recall:  0.6879815100154083\n",
      "==================\n",
      "tf acc:  0.6687211093990755\n",
      "tf f1:  0.6659915493079803\n",
      "tf precision:  0.6647184483147467\n",
      "tf recall:  0.6687211093990755\n",
      "==================\n",
      "w2v acc:  0.515793528505393\n",
      "w2v f1:  0.4996253017823105\n",
      "w2v precision:  0.4955996903124136\n",
      "w2v recall:  0.515793528505393\n",
      "==================\n",
      "lin_svm\n",
      "==================\n",
      "tfidf acc:  0.6798921417565486\n",
      "tfidf f1:  0.6796612464208107\n",
      "tfidf precision:  0.6851511607249138\n",
      "tfidf recall:  0.6798921417565486\n",
      "==================\n",
      "tf acc:  0.6579352850539292\n",
      "tf f1:  0.6544778325210349\n",
      "tf precision:  0.6544647966503119\n",
      "tf recall:  0.6579352850539292\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf acc:  0.6941448382126348\n",
      "tfidf f1:  0.6929851460311124\n",
      "tfidf precision:  0.704506386175642\n",
      "tfidf recall:  0.6941448382126348\n",
      "==================\n",
      "tf acc:  0.714175654853621\n",
      "tf f1:  0.7137245090277426\n",
      "tf precision:  0.7245950249260565\n",
      "tf recall:  0.714175654853621\n",
      "==================\n",
      "w2v acc:  0.5100154083204931\n",
      "w2v f1:  0.4332572921128232\n",
      "w2v precision:  0.41024152780596407\n",
      "w2v recall:  0.5100154083204931\n",
      "==================\n",
      "tree\n",
      "==================\n",
      "tfidf acc:  0.6302003081664098\n",
      "tfidf f1:  0.6289259312408813\n",
      "tfidf precision:  0.6278195501435767\n",
      "tfidf recall:  0.6302003081664098\n",
      "==================\n",
      "tf acc:  0.5982280431432974\n",
      "tf f1:  0.5963046184313373\n",
      "tf precision:  0.5948005377327362\n",
      "tf recall:  0.5982280431432974\n",
      "==================\n",
      "w2v acc:  0.5778120184899846\n",
      "w2v f1:  0.579058141466949\n",
      "w2v precision:  0.5807109296887204\n",
      "w2v recall:  0.5778120184899846\n",
      "==================\n",
      "rf\n",
      "==================\n",
      "tfidf acc:  0.687211093990755\n",
      "tfidf f1:  0.6795152273421196\n",
      "tfidf precision:  0.6789466187123883\n",
      "tfidf recall:  0.687211093990755\n",
      "==================\n",
      "tf acc:  0.6263482280431433\n",
      "tf f1:  0.6147788531770817\n",
      "tf precision:  0.6289861150700086\n",
      "tf recall:  0.6263482280431433\n",
      "==================\n",
      "w2v acc:  0.6398305084745762\n",
      "w2v f1:  0.6376798927907322\n",
      "w2v precision:  0.6381051202599184\n",
      "w2v recall:  0.6398305084745762\n",
      "==================\n",
      "ada\n",
      "==================\n",
      "tfidf acc:  0.5473805855161787\n",
      "tfidf f1:  0.5262996040041493\n",
      "tfidf precision:  0.5327677493669328\n",
      "tfidf recall:  0.5473805855161787\n",
      "==================\n",
      "tf acc:  0.5670261941448382\n",
      "tf f1:  0.5388070054332165\n",
      "tf precision:  0.5626726131804644\n",
      "tf recall:  0.5670261941448382\n",
      "==================\n",
      "w2v acc:  0.5030816640986132\n",
      "w2v f1:  0.46364210329492683\n",
      "w2v precision:  0.48995611936281663\n",
      "w2v recall:  0.5030816640986132\n",
      "==================\n",
      "gb\n",
      "==================\n",
      "tfidf acc:  0.7126348228043143\n",
      "tfidf f1:  0.7112178825280995\n",
      "tfidf precision:  0.7124145728645078\n",
      "tfidf recall:  0.7126348228043143\n",
      "==================\n",
      "tf acc:  0.6671802773497689\n",
      "tf f1:  0.6644227352785693\n",
      "tf precision:  0.6740290954607238\n",
      "tf recall:  0.6671802773497689\n",
      "==================\n",
      "w2v acc:  0.6771956856702619\n",
      "w2v f1:  0.6786444062419466\n",
      "w2v precision:  0.6813704223813889\n",
      "w2v recall:  0.6771956856702619\n",
      "==================\n",
      "xgb\n",
      "==================\n",
      "tfidf acc:  0.7245762711864406\n",
      "tfidf f1:  0.7223852160700287\n",
      "tfidf precision:  0.7212926330136676\n",
      "tfidf recall:  0.7245762711864406\n",
      "==================\n",
      "tf acc:  0.6902927580893683\n",
      "tf f1:  0.6875645288410488\n",
      "tf precision:  0.6908460095021359\n",
      "tf recall:  0.6902927580893683\n",
      "==================\n",
      "w2v acc:  0.6683359013867488\n",
      "w2v f1:  0.6687390845707359\n",
      "w2v precision:  0.670154639079662\n",
      "w2v recall:  0.6683359013867488\n",
      "==================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are the best results from the training above.\r\n",
    "\r\n",
    "### log_reg\r\n",
    "- tfidf acc:  0.6879815100154083\r\n",
    "- tfidf f1:  0.6826731058258083\r\n",
    "- tfidf precision:  0.6840742515662983\r\n",
    "- tfidf recall:  0.6879815100154083\r\n",
    "\r\n",
    "### lin_svm\r\n",
    "- tfidf acc:  0.6798921417565486\r\n",
    "- tfidf f1:  0.6796612464208107\r\n",
    "- tfidf precision:  0.6851511607249138\r\n",
    "- tfidf recall:  0.6798921417565486\r\n",
    "\r\n",
    "### rbf_svm\r\n",
    "- tf acc:  0.714175654853621\r\n",
    "- tf f1:  0.7137245090277426\r\n",
    "- tf precision:  0.7245950249260565\r\n",
    "- tf recall:  0.714175654853621\r\n",
    "\r\n",
    "### tree\r\n",
    "- tfidf acc:  0.6302003081664098\r\n",
    "- tfidf f1:  0.6289259312408813\r\n",
    "- tfidf precision:  0.6278195501435767\r\n",
    "- tfidf recall:  0.6302003081664098\r\n",
    "\r\n",
    "### rf\r\n",
    "- tfidf acc:  0.687211093990755\r\n",
    "- tfidf f1:  0.6795152273421196\r\n",
    "- tfidf precision:  0.6789466187123883\r\n",
    "- tfidf recall:  0.687211093990755\r\n",
    "\r\n",
    "### ada\r\n",
    "- tf acc:  0.5670261941448382\r\n",
    "- tf f1:  0.5388070054332165\r\n",
    "- tf precision:  0.5626726131804644\r\n",
    "- tf recall:  0.5670261941448382\r\n",
    "\r\n",
    "\r\n",
    "### gb\r\n",
    "- tfidf acc:  0.7126348228043143\r\n",
    "- tfidf f1:  0.7112178825280995\r\n",
    "- tfidf precision:  0.7124145728645078\r\n",
    "- tfidf recall:  0.7126348228043143\r\n",
    "\r\n",
    "### These were the best of the word2vec models (Gradient boosting)\r\n",
    "- w2v acc:  0.6771956856702619\r\n",
    "- w2v f1:  0.6786444062419466\r\n",
    "- w2v precision:  0.6813704223813889\r\n",
    "- w2v recall:  0.6771956856702619\r\n",
    "\r\n",
    "### xgb\r\n",
    "- tfidf acc:  0.7245762711864406\r\n",
    "- tfidf f1:  0.7223852160700287\r\n",
    "- tfidf precision:  0.7212926330136676\r\n",
    "- tfidf recall:  0.7245762711864406\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for key in classifiers.keys():\r\n",
    "    try:\r\n",
    "        print(key)\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"tfidf cm: \\n\", tfidf_res[key]['cm'])\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"tf cm: \\n\", tf_res[key]['cm'])\r\n",
    "        print(\"==================\")\r\n",
    "        print(\"w2v cm: \\n\", w2v_res[key]['cm'])\r\n",
    "    except:\r\n",
    "        pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf acc: \n",
      " [[853  53  28   0   3]\n",
      " [405  88   7   5  13]\n",
      " [252  22  91   6  37]\n",
      " [ 62  13   2   5   0]\n",
      " [161  27  45   2 416]]\n",
      "==================\n",
      "tf acc: \n",
      " [[929   6   2   0   0]\n",
      " [498  18   0   0   2]\n",
      " [343   5  54   0   6]\n",
      " [ 77   5   0   0   0]\n",
      " [270  16  55   0 310]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[568 168 133   7  61]\n",
      " [240 190  23  24  41]\n",
      " [192  44 133   7  32]\n",
      " [ 27  32  17   5   1]\n",
      " [123  58  52   1 417]]\n",
      "nb\n",
      "==================\n",
      "tfidf acc: \n",
      " [[590 182 138  17  10]\n",
      " [161 281   0  73   3]\n",
      " [124  42 197  45   0]\n",
      " [ 19  13   0  49   1]\n",
      " [ 58  97  95  28 373]]\n",
      "==================\n",
      "tf acc: \n",
      " [[499 215 164  54   5]\n",
      " [103 259   0 156   0]\n",
      " [ 79  31 229  69   0]\n",
      " [ 10  16   0  56   0]\n",
      " [ 34 106 107  32 372]]\n",
      "==================\n",
      "w2v acc: \n",
      " 0\n",
      "log_reg\n",
      "==================\n",
      "tfidf acc: \n",
      " [[788  81  53   1  14]\n",
      " [151 312   7   6  42]\n",
      " [ 90  72 203   6  37]\n",
      " [  8  39  12  14   9]\n",
      " [ 22  59  88  13 469]]\n",
      "==================\n",
      "tf acc: \n",
      " [[752  94  72   5  14]\n",
      " [145 261  35  22  55]\n",
      " [ 89  49 209  13  48]\n",
      " [  6  24  23  19  10]\n",
      " [ 19  50  76  11 495]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[589 205  84   0  59]\n",
      " [246 208   0   0  64]\n",
      " [168  38 115   0  87]\n",
      " [ 27  12   0   0  43]\n",
      " [133  44  47   0 427]]\n",
      "lin_svm\n",
      "==================\n",
      "tfidf acc: \n",
      " [[756 102  66   0  13]\n",
      " [136 326   7  17  32]\n",
      " [ 88  76 201   8  35]\n",
      " [  8  35  15  21   3]\n",
      " [ 19  63  92  16 461]]\n",
      "==================\n",
      "tf acc: \n",
      " [[759  95  68   2  13]\n",
      " [169 252  33  21  43]\n",
      " [ 89  51 201  18  49]\n",
      " [  6  30  22  18   6]\n",
      " [ 35  50  78  10 478]]\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf acc: \n",
      " [[765 104  59   0   9]\n",
      " [129 361   1   6  21]\n",
      " [ 89  86 206   6  21]\n",
      " [ 12  47   8  14   1]\n",
      " [ 18  67  92  18 456]]\n",
      "==================\n",
      "tf acc: \n",
      " [[781 101  47   0   8]\n",
      " [123 366   0   4  25]\n",
      " [ 68  88 221   7  24]\n",
      " [  5  49   8  18   2]\n",
      " [ 13  67  87  16 468]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[786  40   0   0 111]\n",
      " [277 103   0   0 138]\n",
      " [340  41   0   0  27]\n",
      " [ 25  29   0   0  28]\n",
      " [195  21   0   0 435]]\n",
      "tree\n",
      "==================\n",
      "tfidf acc: \n",
      " [[707 119  76   7  28]\n",
      " [132 246  60  21  59]\n",
      " [ 86  53 179  25  65]\n",
      " [  6  22  22  15  17]\n",
      " [ 26  59  60  17 489]]\n",
      "==================\n",
      "tf acc: \n",
      " [[670 141  85   8  33]\n",
      " [157 230  43  28  60]\n",
      " [103  40 176  16  73]\n",
      " [  8  28  19  15  12]\n",
      " [ 44  71  65   9 462]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[599 162 122  12  42]\n",
      " [141 232  51  28  66]\n",
      " [106  38 175  21  68]\n",
      " [  7  30  17  16  12]\n",
      " [ 45  48  68  12 478]]\n",
      "rf\n",
      "==================\n",
      "tfidf acc: \n",
      " [[783  86  53   0  15]\n",
      " [173 280  18   9  38]\n",
      " [102  66 189   9  42]\n",
      " [ 12  31  11  17  11]\n",
      " [ 18  51  60   7 515]]\n",
      "==================\n",
      "tf acc: \n",
      " [[798  82  46   0  11]\n",
      " [245 229  14   5  25]\n",
      " [153  57 167   8  23]\n",
      " [ 21  40  10   7   4]\n",
      " [ 52  88  82   4 425]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[718 113  78   6  22]\n",
      " [163 258  29  20  48]\n",
      " [115  50 189  14  40]\n",
      " [ 12  27  20  19   4]\n",
      " [ 32  54  80   8 477]]\n",
      "ada\n",
      "==================\n",
      "tfidf acc: \n",
      " [[629  74  73   7 154]\n",
      " [215 154  19  35  95]\n",
      " [ 91  50  92  27 148]\n",
      " [  7  23  19  28   5]\n",
      " [ 66  21  25  21 518]]\n",
      "==================\n",
      "tf acc: \n",
      " [[816  62  35   8  16]\n",
      " [317 128  23  32  18]\n",
      " [187  43  89  25  64]\n",
      " [ 15  28   8  22   9]\n",
      " [138  23  55  18 417]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[728  45  30  13 121]\n",
      " [276 103  20  43  76]\n",
      " [291  44  41  25   7]\n",
      " [  8  38   6  23   7]\n",
      " [201   5  18  16 411]]\n",
      "gb\n",
      "==================\n",
      "tfidf acc: \n",
      " [[777  88  55   2  15]\n",
      " [138 322   9  17  32]\n",
      " [ 74  61 211  22  40]\n",
      " [ 12  24  11  27   8]\n",
      " [ 14  43  62  19 513]]\n",
      "==================\n",
      "tf acc: \n",
      " [[777  97  51   0  12]\n",
      " [176 301   8  10  23]\n",
      " [104  64 206  13  21]\n",
      " [ 13  41  13  11   4]\n",
      " [ 36  72  85  21 437]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[714 120  71   5  27]\n",
      " [122 317   8  21  50]\n",
      " [ 87  57 209  21  34]\n",
      " [ 16  24  10  29   3]\n",
      " [ 15  46  85  16 489]]\n",
      "xgb\n",
      "==================\n",
      "tfidf acc: \n",
      " [[780 100  45   1  11]\n",
      " [124 323  24  13  34]\n",
      " [ 60  59 227  18  44]\n",
      " [  2  29  19  19  13]\n",
      " [ 11  36  58  14 532]]\n",
      "==================\n",
      "tf acc: \n",
      " [[779 102  43   0  13]\n",
      " [148 309  20  11  30]\n",
      " [ 84  64 222   8  30]\n",
      " [  7  37  19  12   7]\n",
      " [ 17  63  87  14 470]]\n",
      "==================\n",
      "w2v acc: \n",
      " [[715 129  71   4  18]\n",
      " [125 297  32  15  49]\n",
      " [ 77  58 209  15  49]\n",
      " [  5  26  25  19   7]\n",
      " [ 15  51  82   8 495]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# check punctuation\r\n",
    "def check_punctuation(raw_text, punctuation):\r\n",
    "    article = raw_text.split(str(punctuation))\r\n",
    "    for a in article:\r\n",
    "        a = a.strip()\r\n",
    "    sentences = []\r\n",
    "    for sentence in article:\r\n",
    "        print(sentence)\r\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\r\n",
    "        sentences.pop() \r\n",
    "    \r\n",
    "    return sentences"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "e301189debc5177601bb5c59a11b2befed8768132b1b8ef50f2e30593072dd97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}