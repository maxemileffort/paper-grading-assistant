{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Grading Assistant\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Data comes from this link:\n",
    "- https://www.kaggle.com/c/asap-aes/data\n",
    "\n",
    "Heavy inspiration drawn from:\n",
    "- https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "(Use incognito window when opening that link)\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook is the part of the grading process where a teacher might categorize his or her students' papers by letter grade.\n",
    "\n",
    "The idea here is that the teacher will only need to adjust a few grades instead of having to grade an entire stack of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import os, sys\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxw2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run the utilty functions from a seperate notebook\n",
    "%run topic_model_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\Kaggle\\\\asap-aes\\\\training_set_rel3.tsv\", sep='\\t')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_essay'] = data.essay.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>tokenized_essay</th>\n",
       "      <th>max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, believe, using, computer, benefit, way,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, people, use, computer, agrees, benefit,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, local, newspaper, expert, computer, ben...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, location, know, having, computer, posit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             0.0              8   \n",
       "1               5               4             0.0              9   \n",
       "2               4               3             0.0              7   \n",
       "3               5               5             0.0             10   \n",
       "4               4               4             0.0              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait5  \\\n",
       "0             0.0             0.0            0.0  ...            0.0   \n",
       "1             0.0             0.0            0.0  ...            0.0   \n",
       "2             0.0             0.0            0.0  ...            0.0   \n",
       "3             0.0             0.0            0.0  ...            0.0   \n",
       "4             0.0             0.0            0.0  ...            0.0   \n",
       "\n",
       "   rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   rater3_trait5  rater3_trait6  \\\n",
       "0            0.0            0.0   \n",
       "1            0.0            0.0   \n",
       "2            0.0            0.0   \n",
       "3            0.0            0.0   \n",
       "4            0.0            0.0   \n",
       "\n",
       "                                     tokenized_essay  max_score  \n",
       "0  [dear, local, newspaper, think, effect, comput...          0  \n",
       "1  [dear, believe, using, computer, benefit, way,...          0  \n",
       "2  [dear, people, use, computer, agrees, benefit,...          0  \n",
       "3  [dear, local, newspaper, expert, computer, ben...          0  \n",
       "4  [dear, location, know, having, computer, posit...          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN w/ 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "# add a max_score column to use later \n",
    "# for standardizing scores, as all the \n",
    "# different essays sets have different \n",
    "# scales on which they were scored\n",
    "data['max_score'] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change max score col based on essay set\n",
    "# max vals:\n",
    "# set 1: 12\n",
    "# set 2: 10 or 24, needs some experimenting\n",
    "# set 3: 3\n",
    "# set 4: 3\n",
    "# set 5: 4\n",
    "# set 6: 4\n",
    "# set 7: 30\n",
    "# set 8: 60\n",
    "\n",
    "essay_sets = data.essay_set.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       12\n",
      "1       12\n",
      "2       12\n",
      "3       12\n",
      "4       12\n",
      "        ..\n",
      "1778    12\n",
      "1779    12\n",
      "1780    12\n",
      "1781    12\n",
      "1782    12\n",
      "Name: max_score, Length: 1783, dtype: int64\n",
      "5309    3\n",
      "5310    3\n",
      "5311    3\n",
      "5312    3\n",
      "5313    3\n",
      "       ..\n",
      "7074    3\n",
      "7075    3\n",
      "7076    3\n",
      "7077    3\n",
      "7078    3\n",
      "Name: max_score, Length: 1770, dtype: int64\n",
      "10684    30\n",
      "10685    30\n",
      "10686    30\n",
      "10687    30\n",
      "10688    30\n",
      "         ..\n",
      "12248    30\n",
      "12249    30\n",
      "12250    30\n",
      "12251    30\n",
      "12252    30\n",
      "Name: max_score, Length: 1569, dtype: int64\n",
      "12253    60\n",
      "12254    60\n",
      "12255    60\n",
      "12256    60\n",
      "12257    60\n",
      "         ..\n",
      "12971    60\n",
      "12972    60\n",
      "12973    60\n",
      "12974    60\n",
      "12975    60\n",
      "Name: max_score, Length: 723, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set_ in essay_sets:\n",
    "    if set_ == 1:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 12\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 10\n",
    "    if set_ == 3 or set_ == 4:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 3\n",
    "    if set_ == 5 or set_ == 6:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 4\n",
    "    if set_ == 7:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 30\n",
    "    if set_ == 8:\n",
    "        data.loc[data.essay_set == set_, 'max_score'] = 60\n",
    "# spot checking some of the data\n",
    "print(data.loc[data.essay_set == 1, 'max_score'])\n",
    "print(data.loc[data.essay_set == 4, 'max_score'])\n",
    "print(data.loc[data.essay_set == 7, 'max_score'])\n",
    "print(data.loc[data.essay_set == 8, 'max_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp column for \n",
    "# model's later internal classes\n",
    "data['temp'] = 0\n",
    "for set_ in essay_sets:\n",
    "    if set_ == 2:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = (data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   + data.loc[data.essay_set==set_,'domain2_score']) \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']\n",
    "        continue\n",
    "    else:\n",
    "        data.loc[data.essay_set == set_, 'temp'] = data.loc[data.essay_set==set_,'domain1_score'] \\\n",
    "                                                   / data.loc[data.essay_set==set_,'max_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-cebdbcdee144>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 2\n",
      "<ipython-input-9-cebdbcdee144>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 3\n",
      "<ipython-input-9-cebdbcdee144>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 4\n",
      "<ipython-input-9-cebdbcdee144>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['class'][x] = 5\n"
     ]
    }
   ],
   "source": [
    "# re-classify each paper on a scale of 1-5,\n",
    "# with 5 being a high score (like an A on an \n",
    "# ABCDF scale)\n",
    "data['class'] = 1\n",
    "for x in range(len(data)):\n",
    "    if (data.temp[x]) >= .9:\n",
    "        data['class'][x] = 5\n",
    "        continue\n",
    "    elif data.temp[x] >= .8 and data.temp[x] < .9:\n",
    "        data['class'][x] = 4\n",
    "        continue\n",
    "    elif data.temp[x] >= .7 and data.temp[x] < .8:\n",
    "        data['class'][x] = 3\n",
    "        continue\n",
    "    elif data.temp[x] >= .6 and data.temp[x] < .7:\n",
    "        data['class'][x] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# Initialize tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, \n",
    "                                   min_df=3, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english', \n",
    "                                   preprocessor=' '.join)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Bag of words\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, \n",
    "                                min_df=3, \n",
    "                                max_features=no_features, \n",
    "                                stop_words='english', \n",
    "                                preprocessor=' '.join)\n",
    "tf = tf_vectorizer.fit_transform(data['tokenized_essay'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Word2Vec\n",
    "word2vec = WordEmbeddingsService()\n",
    "word2vec_model = word2vec.train_w2v_model(tokenized_text=data['tokenized_essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a few different vecotrizations of the data\n",
    "# to see which version does the best\n",
    "\n",
    "X_tfidf = tfidf\n",
    "X_tf = tf\n",
    "X_w2v = word2vec.create_word_embeddings(data['tokenized_essay'], word2vec_model)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the different classifiers \n",
    "# to test with the paper scores\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(classifier, X, y, rs=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm, acc_score, prec_score, rec_score = make_confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, f1, prec_score, rec_score\n",
    "\n",
    "def make_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    prec_score = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec_score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return cm, acc_score, prec_score, rec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all the different classifiers\n",
    "# to loop through.\n",
    "# There are some unsupervised models just for comparison.\n",
    "classifiers = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2),\n",
    "    \"nb\" : MultinomialNB(), \n",
    "    \"log_reg\": LogisticRegression(random_state=0),\n",
    "    \"lin_svm\" : SVC(kernel = 'linear', random_state = 0), # took too long with word2vec (more than 5000 secs)\n",
    "    \"rbf_svm\" : SVC(kernel = 'rbf', random_state = 0),\n",
    "    \"tree\" : DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\n",
    "    \"rf\" : RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\n",
    "    \"ada\" : AdaBoostClassifier(random_state = 0),\n",
    "    \"gb\" : GradientBoostingClassifier(random_state = 0),\n",
    "    \"xgb\" : XGBClassifier(random_state = 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:01:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectors first, 3 min\n",
    "tfidf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tfidf, y)\n",
    "    tfidf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }\n",
    "    print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "nb\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "lin_svm\n",
      "==============\n",
      "rbf_svm\n",
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n",
      "[13:04:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with bag of words models, 2.5 min\n",
    "tf_res = {}\n",
    "for key in classifiers.keys():\n",
    "    print(key)\n",
    "    cm, acc, f1, prec, rec = make_classification(classifiers[key], X_tf, y)\n",
    "    print(\"==============\")\n",
    "    tf_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==============\n",
      "log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "rbf_svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "tree\n",
      "==============\n",
      "rf\n",
      "==============\n",
      "ada\n",
      "==============\n",
      "gb\n",
      "==============\n",
      "xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxw2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# repeat classification with word2vec models, 5 min\n",
    "w2v_res = {}\n",
    "for key in classifiers.keys():\n",
    "    # lin_svm takes more than 1 hour on its own.\n",
    "    # nb doesn't accept negative numbers from the vectors.\n",
    "    if key == 'lin_svm' or key == 'nb': \n",
    "        continue\n",
    "    print(key)\n",
    "    try:\n",
    "        cm, acc, f1, prec, rec = make_classification(classifiers[key], X_w2v, y)\n",
    "    except:\n",
    "        cm, acc, f1, prec, rec = 0,0,0,0,0\n",
    "    print(\"==============\")\n",
    "    w2v_res[key] = {\n",
    "        'cm' : cm,\n",
    "        'acc' : acc,\n",
    "        'f1' : f1,\n",
    "        'prec' : prec,\n",
    "        'rec' : rec\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf acc:  0.4518489984591679\n",
      "tfidf f1:  0.3845584550763674\n",
      "tfidf precision:  0.4528836182166853\n",
      "tfidf recall:  0.4518489984591679\n",
      "==================\n",
      "tf acc:  0.41756548536209553\n",
      "tf f1:  0.2887254767695843\n",
      "tf precision:  0.4608517797915959\n",
      "tf recall:  0.41756548536209553\n",
      "==================\n",
      "w2v acc:  0.40331278890600925\n",
      "w2v f1:  0.389688959015477\n",
      "w2v precision:  0.3850056213809215\n",
      "w2v recall:  0.40331278890600925\n",
      "==================\n",
      "nb\n",
      "==================\n",
      "tfidf acc:  0.4872881355932203\n",
      "tfidf f1:  0.4727566874766083\n",
      "tfidf precision:  0.5062737992494357\n",
      "tfidf recall:  0.4872881355932203\n",
      "==================\n",
      "tf acc:  0.4476117103235747\n",
      "tf f1:  0.4522116160194951\n",
      "tf precision:  0.4892058007112515\n",
      "tf recall:  0.4476117103235747\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf acc:  0.6171032357473035\n",
      "tfidf f1:  0.6096852266087265\n",
      "tfidf precision:  0.6113537557873977\n",
      "tfidf recall:  0.6171032357473035\n",
      "==================\n",
      "tf acc:  0.5970724191063174\n",
      "tf f1:  0.5930907314611129\n",
      "tf precision:  0.5908905542533972\n",
      "tf recall:  0.5970724191063174\n",
      "==================\n",
      "w2v acc:  0.45454545454545453\n",
      "w2v f1:  0.4256449606103841\n",
      "w2v precision:  0.4129149380725789\n",
      "w2v recall:  0.45454545454545453\n",
      "==================\n",
      "lin_svm\n",
      "==================\n",
      "tfidf acc:  0.613251155624037\n",
      "tfidf f1:  0.608159930006525\n",
      "tfidf precision:  0.6131331303612533\n",
      "tfidf recall:  0.613251155624037\n",
      "==================\n",
      "tf acc:  0.5955315870570108\n",
      "tf f1:  0.5897598019549904\n",
      "tf precision:  0.5872646490162303\n",
      "tf recall:  0.5955315870570108\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf acc:  0.6302003081664098\n",
      "tfidf f1:  0.622295819186602\n",
      "tfidf precision:  0.6346727764680054\n",
      "tfidf recall:  0.6302003081664098\n",
      "==================\n",
      "tf acc:  0.6475346687211094\n",
      "tf f1:  0.6415366136770412\n",
      "tf precision:  0.6535206117830535\n",
      "tf recall:  0.6475346687211094\n",
      "==================\n",
      "w2v acc:  0.44414483821263484\n",
      "w2v f1:  0.340410751866202\n",
      "w2v precision:  0.27669044799694253\n",
      "w2v recall:  0.44414483821263484\n",
      "==================\n",
      "tree\n",
      "==================\n",
      "tfidf acc:  0.5520030816640986\n",
      "tfidf f1:  0.5528056512924708\n",
      "tfidf precision:  0.5539368422055277\n",
      "tfidf recall:  0.5520030816640986\n",
      "==================\n",
      "tf acc:  0.5161787365177196\n",
      "tf f1:  0.512722923024526\n",
      "tf precision:  0.5100858491357774\n",
      "tf recall:  0.5161787365177196\n",
      "==================\n",
      "w2v acc:  0.4772727272727273\n",
      "w2v f1:  0.47835557207866825\n",
      "w2v precision:  0.47964911833163604\n",
      "w2v recall:  0.4772727272727273\n",
      "==================\n",
      "rf\n",
      "==================\n",
      "tfidf acc:  0.6147919876733436\n",
      "tfidf f1:  0.605882006540668\n",
      "tfidf precision:  0.6049429246696592\n",
      "tfidf recall:  0.6147919876733436\n",
      "==================\n",
      "tf acc:  0.5469953775038521\n",
      "tf f1:  0.5230809175023926\n",
      "tf precision:  0.5359206345563959\n",
      "tf recall:  0.5469953775038521\n",
      "==================\n",
      "w2v acc:  0.5658705701078582\n",
      "w2v f1:  0.5605980172190496\n",
      "w2v precision:  0.5588383915163577\n",
      "w2v recall:  0.5658705701078582\n",
      "==================\n",
      "ada\n",
      "==================\n",
      "tfidf acc:  0.4930662557781202\n",
      "tfidf f1:  0.49476259171788406\n",
      "tfidf precision:  0.5013301497500575\n",
      "tfidf recall:  0.4930662557781202\n",
      "==================\n",
      "tf acc:  0.49768875192604006\n",
      "tf f1:  0.4845876911067036\n",
      "tf precision:  0.48788715673893546\n",
      "tf recall:  0.49768875192604006\n",
      "==================\n",
      "w2v acc:  0.4379815100154083\n",
      "w2v f1:  0.36992556285572403\n",
      "w2v precision:  0.41017589316380026\n",
      "w2v recall:  0.4379815100154083\n",
      "==================\n",
      "gb\n",
      "==================\n",
      "tfidf acc:  0.6348228043143297\n",
      "tfidf f1:  0.6312471077545355\n",
      "tfidf precision:  0.6348880983202261\n",
      "tfidf recall:  0.6348228043143297\n",
      "==================\n",
      "tf acc:  0.5924499229583975\n",
      "tf f1:  0.5790485353216255\n",
      "tf precision:  0.5897744296819609\n",
      "tf recall:  0.5924499229583975\n",
      "==================\n",
      "w2v acc:  0.5963020030816641\n",
      "w2v f1:  0.594721033739939\n",
      "w2v precision:  0.5999014604841247\n",
      "w2v recall:  0.5963020030816641\n",
      "==================\n",
      "xgb\n",
      "==================\n",
      "tfidf acc:  0.6432973805855162\n",
      "tfidf f1:  0.6412741241938258\n",
      "tfidf precision:  0.642001655993716\n",
      "tfidf recall:  0.6432973805855162\n",
      "==================\n",
      "tf acc:  0.6171032357473035\n",
      "tf f1:  0.6106161331342493\n",
      "tf precision:  0.6128254371185095\n",
      "tf recall:  0.6171032357473035\n",
      "==================\n",
      "w2v acc:  0.5843605546995377\n",
      "w2v f1:  0.5839557780109441\n",
      "w2v precision:  0.584194978036909\n",
      "w2v recall:  0.5843605546995377\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# everything else being equal,\n",
    "# we want the one with highest precisions \n",
    "# (precision is affected by FP, which would be \n",
    "# overestimation of the grade of the paper)\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf acc: \", tfidf_res[key]['acc'])\n",
    "        print(\"tfidf f1: \", tfidf_res[key]['f1'])\n",
    "        print(\"tfidf precision: \", tfidf_res[key]['prec'])\n",
    "        print(\"tfidf recall: \", tfidf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf acc: \", tf_res[key]['acc'])\n",
    "        print(\"tf f1: \", tf_res[key]['f1'])\n",
    "        print(\"tf precision: \", tf_res[key]['prec'])\n",
    "        print(\"tf recall: \", tf_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v acc: \", w2v_res[key]['acc'])\n",
    "        print(\"w2v f1: \", w2v_res[key]['f1'])\n",
    "        print(\"w2v precision: \", w2v_res[key]['prec'])\n",
    "        print(\"w2v recall: \", w2v_res[key]['rec'])\n",
    "        print(\"==================\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the best results from the training above. \n",
    "\n",
    "*Note:* I left out the unsupervised learning models because I generally just like to test them for a \"shot in the dark\" type of look at finding the optimal model. I attribute this to a short stent as a marketer where testing EVERYTHING was an important part of the puzzle.\n",
    "\n",
    "### log_reg\n",
    "- tfidf acc:  0.6171032357473035\n",
    "- tfidf f1:  0.6096852266087265\n",
    "- tfidf precision:  0.6113537557873977\n",
    "- tfidf recall:  0.6171032357473035\n",
    "\n",
    "### lin_svm\n",
    "- tfidf acc:  0.613251155624037\n",
    "- tfidf f1:  0.608159930006525\n",
    "- tfidf precision:  0.6131331303612533\n",
    "- tfidf recall:  0.613251155624037\n",
    "\n",
    "### rbf_svm\n",
    "- tf acc:  0.6475346687211094\n",
    "- tf f1:  0.6415366136770412\n",
    "- tf precision:  0.6535206117830535\n",
    "- tf recall:  0.6475346687211094\n",
    "\n",
    "### tree\n",
    "- tfidf acc:  0.5520030816640986\n",
    "- tfidf f1:  0.5528056512924708\n",
    "- tfidf precision:  0.5539368422055277\n",
    "- tfidf recall:  0.5520030816640986\n",
    "\n",
    "### rf\n",
    "- tfidf acc:  0.6147919876733436\n",
    "- tfidf f1:  0.605882006540668\n",
    "- tfidf precision:  0.6049429246696592\n",
    "- tfidf recall:  0.6147919876733436\n",
    "\n",
    "### ada\n",
    "- tf acc:  0.49768875192604006\n",
    "- tf f1:  0.4845876911067036\n",
    "- tf precision:  0.48788715673893546\n",
    "- tf recall:  0.49768875192604006\n",
    "\n",
    "### gb\n",
    "- tfidf acc:  0.6348228043143297\n",
    "- tfidf f1:  0.6312471077545355\n",
    "- tfidf precision:  0.6348880983202261\n",
    "- tfidf recall:  0.6348228043143297\n",
    "\n",
    "### best word2vec model results (gradient boost)\n",
    "- w2v acc:  0.5963020030816641\n",
    "- w2v f1:  0.594721033739939\n",
    "- w2v precision:  0.5999014604841247\n",
    "- w2v recall:  0.5963020030816641\n",
    "\n",
    "### xgb\n",
    "- tfidf acc:  0.6432973805855162\n",
    "- tfidf f1:  0.6412741241938258\n",
    "- tfidf precision:  0.642001655993716\n",
    "- tfidf recall:  0.6432973805855162\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "==================\n",
      "tfidf cm: \n",
      " [[897  62  34   1   3]\n",
      " [499  98  11  16  13]\n",
      " [295  29  99  17  37]\n",
      " [133  17   7  31   0]\n",
      " [166  26  48   9  48]]\n",
      "==================\n",
      "tf cm: \n",
      " [[986   6   5   0   0]\n",
      " [608  27   1   0   1]\n",
      " [401  12  58   0   6]\n",
      " [155  25   6   2   0]\n",
      " [210  21  55   0  11]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[593 186 137  25  56]\n",
      " [270 231  42  55  39]\n",
      " [201  70 145  28  33]\n",
      " [ 47  68  38  34   1]\n",
      " [126  65  56   6  44]]\n",
      "nb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[598 204 136  48  11]\n",
      " [180 308   0 136  13]\n",
      " [125  70 197  85   0]\n",
      " [ 20  29   0 139   0]\n",
      " [ 68  73  89  44  23]]\n",
      "==================\n",
      "tf cm: \n",
      " [[530 168 147  88  64]\n",
      " [138 190   0 219  90]\n",
      " [104  45 201 115  12]\n",
      " [ 16  14   1 157   0]\n",
      " [ 43  35  88  47  84]]\n",
      "==================\n",
      "log_reg\n",
      "==================\n",
      "tfidf cm: \n",
      " [[813 120  53   2   9]\n",
      " [170 366  23  40  38]\n",
      " [ 94  95 221  41  26]\n",
      " [  7  52  24  99   6]\n",
      " [ 14  54  92  34 103]]\n",
      "==================\n",
      "tf cm: \n",
      " [[802 117  58   8  12]\n",
      " [163 315  58  47  54]\n",
      " [ 90  74 210  49  54]\n",
      " [  7  40  32  86  23]\n",
      " [ 10  54  73  23 137]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[649 214  88  36  10]\n",
      " [274 274   4  85   0]\n",
      " [161  80 184  50   2]\n",
      " [ 27  65  24  72   0]\n",
      " [135  58  92  11   1]]\n",
      "lin_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[787 135  65   1   9]\n",
      " [144 392  25  43  33]\n",
      " [ 84 111 213  39  30]\n",
      " [  9  55  24  98   2]\n",
      " [  9  56  92  38 102]]\n",
      "==================\n",
      "tf cm: \n",
      " [[792 120  71   4  10]\n",
      " [191 331  46  32  37]\n",
      " [ 93  84 214  37  49]\n",
      " [  7  50  45  77   9]\n",
      " [ 24  41  78  22 132]]\n",
      "==================\n",
      "rbf_svm\n",
      "==================\n",
      "tfidf cm: \n",
      " [[797 134  58   1   7]\n",
      " [143 427  11  35  21]\n",
      " [ 87 116 211  42  21]\n",
      " [ 12  59  11 105   1]\n",
      " [ 11  62  86  42  96]]\n",
      "==================\n",
      "tf cm: \n",
      " [[812 128  49   2   6]\n",
      " [140 428  11  33  25]\n",
      " [ 73 117 219  46  22]\n",
      " [  4  57  12 115   0]\n",
      " [  6  53  84  47 107]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[816 181   0   0   0]\n",
      " [300 337   0   0   0]\n",
      " [340 137   0   0   0]\n",
      " [ 25 163   0   0   0]\n",
      " [211  86   0   0   0]]\n",
      "tree\n",
      "==================\n",
      "tfidf cm: \n",
      " [[723 145  89  20  20]\n",
      " [164 297  69  50  57]\n",
      " [ 88  63 212  53  61]\n",
      " [  6  50  54  59  19]\n",
      " [ 16  47  66  26 142]]\n",
      "==================\n",
      "tf cm: \n",
      " [[704 149  97  15  32]\n",
      " [181 284  60  49  63]\n",
      " [120  61 186  42  68]\n",
      " [ 22  54  41  54  17]\n",
      " [ 37  58  68  22 112]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[628 192 107  16  54]\n",
      " [180 264  73  51  69]\n",
      " [120  69 186  40  62]\n",
      " [ 12  43  44  71  18]\n",
      " [ 36  58  95  18  90]]\n",
      "rf\n",
      "==================\n",
      "tfidf cm: \n",
      " [[838  97  47   4  11]\n",
      " [204 332  34  34  33]\n",
      " [101  93 212  38  33]\n",
      " [ 15  57  32  78   6]\n",
      " [  9  52  74  26 136]]\n",
      "==================\n",
      "tf cm: \n",
      " [[824 108  57   3   5]\n",
      " [271 302  20  28  16]\n",
      " [151  99 177  29  21]\n",
      " [ 35  72  22  59   0]\n",
      " [ 42  85  93  19  58]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[759 147  66   9  16]\n",
      " [188 319  36  43  51]\n",
      " [107  70 204  54  42]\n",
      " [ 13  52  36  78   9]\n",
      " [ 24  53  86  25 109]]\n",
      "ada\n",
      "==================\n",
      "tfidf cm: \n",
      " [[596 144 182   8  67]\n",
      " [212 245  41  53  86]\n",
      " [ 81  53 240  48  55]\n",
      " [ 19  30  24  95  20]\n",
      " [ 37  46  77  33 104]]\n",
      "==================\n",
      "tf cm: \n",
      " [[708 163  49  24  53]\n",
      " [199 277  17  63  81]\n",
      " [203  68 123  59  24]\n",
      " [ 21  38  14 101  14]\n",
      " [ 61  50  72  31  83]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[843  66  14   9  65]\n",
      " [359 143   5  68  62]\n",
      " [339  55  17  46  20]\n",
      " [ 26  50   7  83  22]\n",
      " [211   3   5  27  51]]\n",
      "gb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[797 130  57   1  12]\n",
      " [155 387  22  40  33]\n",
      " [ 82  89 215  55  36]\n",
      " [  9  50  10 109  10]\n",
      " [ 13  40  66  38 140]]\n",
      "==================\n",
      "tf cm: \n",
      " [[807 129  51   2   8]\n",
      " [214 351  17  33  22]\n",
      " [119  93 198  47  20]\n",
      " [ 19  58  10 100   1]\n",
      " [ 31  67  79  38  82]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[734 156  77  11  19]\n",
      " [140 391  11  49  46]\n",
      " [ 88  90 206  50  43]\n",
      " [ 11  60  14  98   5]\n",
      " [ 17  53  73  35 119]]\n",
      "xgb\n",
      "==================\n",
      "tfidf cm: \n",
      " [[802 129  51   1  14]\n",
      " [143 388  30  40  36]\n",
      " [ 72  87 223  48  47]\n",
      " [  4  37  37  98  12]\n",
      " [  8  44  57  29 159]]\n",
      "==================\n",
      "tf cm: \n",
      " [[801 134  51   1  10]\n",
      " [175 375  29  26  32]\n",
      " [ 86  96 225  41  29]\n",
      " [  7  47  34  95   5]\n",
      " [ 12  63  86  30 106]]\n",
      "==================\n",
      "w2v cm: \n",
      " [[744 152  76   8  17]\n",
      " [146 346  51  39  55]\n",
      " [ 83  81 218  48  47]\n",
      " [  7  51  44  75  11]\n",
      " [ 19  45  75  24 134]]\n"
     ]
    }
   ],
   "source": [
    "for key in classifiers.keys():\n",
    "    try:\n",
    "        print(key)\n",
    "        print(\"==================\")\n",
    "        print(\"tfidf cm: \\n\", tfidf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"tf cm: \\n\", tf_res[key]['cm'])\n",
    "        print(\"==================\")\n",
    "        print(\"w2v cm: \\n\", w2v_res[key]['cm'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices just for fun. The best models look to be\n",
    "# SVM with rbf kernel and gradient boosting. Now for some cross validation.\n",
    "\n",
    "# rbf svm uses tf\n",
    "# gb uses tfidif\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "X_tfidf = tfidf\n",
    "X_tf = tf\n",
    "y = data['class']\n",
    "\n",
    "svm_X_train, svm_X_test, svm_y_train, svm_y_test = train_test_split(X_tf, \n",
    "                                                                    y, \n",
    "                                                                    test_size = 0.2, \n",
    "                                                                    random_state = 42)\n",
    "\n",
    "gb_X_train, gb_X_test, gb_y_train, gb_y_test = train_test_split(X_tfidf, \n",
    "                                                                y, \n",
    "                                                                test_size = 0.2, \n",
    "                                                                random_state = 42)\n",
    "\n",
    "svm_accuracies = cross_val_score(estimator = classifiers['rbf_svm'], \n",
    "                                 X = svm_X_train, \n",
    "                                 y = svm_y_train, \n",
    "                                 cv = KFold(shuffle=True))\n",
    "\n",
    "gb_accuracies = cross_val_score(estimator = classifiers['gb'], \n",
    "                                 X = gb_X_train, \n",
    "                                 y = gb_y_train, \n",
    "                                 cv = KFold(shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm Accuracies:  0.646917148362235\n",
      "GB Accuracies:  0.6466281310211945\n"
     ]
    }
   ],
   "source": [
    "print(\"svm Accuracies: \", svm_accuracies)\n",
    "print(\"svm Accuracies mean: \", svm_accuracies.mean())\n",
    "print(\"GB Accuracies: \", gb_accuracies)\n",
    "print(\"GB Accuracies mean: \", gb_accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both SVM and Gradient boosting have cross-validation accuracies that are in-line with the initial values.\n",
    "\n",
    "SVM has slightly higher accuracies, and better precision, so it's the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a \"best\" model, it's time to make sure we\n",
    "# are getting the best we can out of it.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gamma_range = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,'scale','auto']\n",
    "c_range = [1e-2,1e0,1e2,1e5]\n",
    "svr_param_grid = {\n",
    "    'kernel' : ('rbf', 'sigmoid'),\n",
    "    'C' : c_range,\n",
    "    'gamma' : gamma_range\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(classifiers['rbf_svm'],svr_param_grid,cv=3,n_jobs=2)\n",
    "gs.fit(X_tf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('svr_cv.best_score_: \\n')\n",
    "print(gs.best_score_)\n",
    "print('svr_cv.best_params_: \\n')\n",
    "print(gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e301189debc5177601bb5c59a11b2befed8768132b1b8ef50f2e30593072dd97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
