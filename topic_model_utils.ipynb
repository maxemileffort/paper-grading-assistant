{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Adapted from https://github.com/robsalgado/personal_data_science_projects/blob/master/topic_modeling_nmf/nlp_topic_utils.ipynb\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\r\n",
    "from sklearn import preprocessing\r\n",
    "import string\r\n",
    "import re\r\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "nltk.download('wordnet')\r\n",
    "\r\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Contraction map\r\n",
    "c_dict = {\r\n",
    "    \"ain't\": \"am not\",\r\n",
    "    \"aren't\": \"are not\",\r\n",
    "    \"can't\": \"cannot\",\r\n",
    "    \"can't've\": \"cannot have\",\r\n",
    "    \"'cause\": \"because\",\r\n",
    "    \"could've\": \"could have\",\r\n",
    "    \"couldn't\": \"could not\",\r\n",
    "    \"couldn't've\": \"could not have\",\r\n",
    "    \"didn't\": \"did not\",\r\n",
    "    \"doesn't\": \"does not\",\r\n",
    "    \"don't\": \"do not\",\r\n",
    "    \"hadn't\": \"had not\",\r\n",
    "    \"hadn't've\": \"had not have\",\r\n",
    "    \"hasn't\": \"has not\",\r\n",
    "    \"haven't\": \"have not\",\r\n",
    "    \"he'd\": \"he would\",\r\n",
    "    \"he'd've\": \"he would have\",\r\n",
    "    \"he'll\": \"he will\",\r\n",
    "    \"he'll've\": \"he will have\",\r\n",
    "    \"he's\": \"he is\",\r\n",
    "    \"how'd\": \"how did\",\r\n",
    "    \"how'd'y\": \"how do you\",\r\n",
    "    \"how'll\": \"how will\",\r\n",
    "    \"how's\": \"how is\",\r\n",
    "    \"i'd\": \"I would\",\r\n",
    "    \"i'd've\": \"I would have\",\r\n",
    "    \"i'll\": \"I will\",\r\n",
    "    \"i'll've\": \"I will have\",\r\n",
    "    \"i'm\": \"I am\",\r\n",
    "    \"i've\": \"I have\",\r\n",
    "    \"isn't\": \"is not\",\r\n",
    "    \"it'd\": \"it had\",\r\n",
    "    \"it'd've\": \"it would have\",\r\n",
    "    \"it'll\": \"it will\",\r\n",
    "    \"it'll've\": \"it will have\",\r\n",
    "    \"it's\": \"it is\",\r\n",
    "    \"let's\": \"let us\",\r\n",
    "    \"ma'am\": \"madam\",\r\n",
    "    \"mayn't\": \"may not\",\r\n",
    "    \"might've\": \"might have\",\r\n",
    "    \"mightn't\": \"might not\",\r\n",
    "    \"mightn't've\": \"might not have\",\r\n",
    "    \"must've\": \"must have\",\r\n",
    "    \"mustn't\": \"must not\",\r\n",
    "    \"mustn't've\": \"must not have\",\r\n",
    "    \"needn't\": \"need not\",\r\n",
    "    \"needn't've\": \"need not have\",\r\n",
    "    \"o'clock\": \"of the clock\",\r\n",
    "    \"oughtn't\": \"ought not\",\r\n",
    "    \"oughtn't've\": \"ought not have\",\r\n",
    "    \"shan't\": \"shall not\",\r\n",
    "    \"sha'n't\": \"shall not\",\r\n",
    "    \"shan't've\": \"shall not have\",\r\n",
    "    \"she'd\": \"she would\",\r\n",
    "    \"she'd've\": \"she would have\",\r\n",
    "    \"she'll\": \"she will\",\r\n",
    "    \"she'll've\": \"she will have\",\r\n",
    "    \"she's\": \"she is\",\r\n",
    "    \"should've\": \"should have\",\r\n",
    "    \"shouldn't\": \"should not\",\r\n",
    "    \"shouldn't've\": \"should not have\",\r\n",
    "    \"so've\": \"so have\",\r\n",
    "    \"so's\": \"so is\",\r\n",
    "    \"that'd\": \"that would\",\r\n",
    "    \"that'd've\": \"that would have\",\r\n",
    "    \"that's\": \"that is\",\r\n",
    "    \"there'd\": \"there had\",\r\n",
    "    \"there'd've\": \"there would have\",\r\n",
    "    \"there's\": \"there is\",\r\n",
    "    \"they'd\": \"they would\",\r\n",
    "    \"they'd've\": \"they would have\",\r\n",
    "    \"they'll\": \"they will\",\r\n",
    "    \"they'll've\": \"they will have\",\r\n",
    "    \"they're\": \"they are\",\r\n",
    "    \"they've\": \"they have\",\r\n",
    "    \"to've\": \"to have\",\r\n",
    "    \"wasn't\": \"was not\",\r\n",
    "    \"we'd\": \"we had\",\r\n",
    "    \"we'd've\": \"we would have\",\r\n",
    "    \"we'll\": \"we will\",\r\n",
    "    \"we'll've\": \"we will have\",\r\n",
    "    \"we're\": \"we are\",\r\n",
    "    \"we've\": \"we have\",\r\n",
    "    \"weren't\": \"were not\",\r\n",
    "    \"what'll\": \"what will\",\r\n",
    "    \"what'll've\": \"what will have\",\r\n",
    "    \"what're\": \"what are\",\r\n",
    "    \"what's\": \"what is\",\r\n",
    "    \"what've\": \"what have\",\r\n",
    "    \"when's\": \"when is\",\r\n",
    "    \"when've\": \"when have\",\r\n",
    "    \"where'd\": \"where did\",\r\n",
    "    \"where's\": \"where is\",\r\n",
    "    \"where've\": \"where have\",\r\n",
    "    \"who'll\": \"who will\",\r\n",
    "    \"who'll've\": \"who will have\",\r\n",
    "    \"who's\": \"who is\",\r\n",
    "    \"who've\": \"who have\",\r\n",
    "    \"why's\": \"why is\",\r\n",
    "    \"why've\": \"why have\",\r\n",
    "    \"will've\": \"will have\",\r\n",
    "    \"won't\": \"will not\",\r\n",
    "    \"won't've\": \"will not have\",\r\n",
    "    \"would've\": \"would have\",\r\n",
    "    \"wouldn't\": \"would not\",\r\n",
    "    \"wouldn't've\": \"would not have\",\r\n",
    "    \"y'all\": \"you all\",\r\n",
    "    \"y'alls\": \"you alls\",\r\n",
    "    \"y'all'd\": \"you all would\",\r\n",
    "    \"y'all'd've\": \"you all would have\",\r\n",
    "    \"y'all're\": \"you all are\",\r\n",
    "    \"y'all've\": \"you all have\",\r\n",
    "    \"you'd\": \"you had\",\r\n",
    "    \"you'd've\": \"you would have\",\r\n",
    "    \"you'll\": \"you you will\",\r\n",
    "    \"you'll've\": \"you you will have\",\r\n",
    "    \"you're\": \"you are\",\r\n",
    "    \"you've\": \"you have\"\r\n",
    "}\r\n",
    "\r\n",
    "# Compiling the contraction dict\r\n",
    "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\r\n",
    "\r\n",
    "# List of stop words\r\n",
    "add_stop = ['said', 'say', '...', 'like', 'ad', 'ha', 'wa', 'reuters', 'just']\r\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\r\n",
    "\r\n",
    "# List of punctuation\r\n",
    "punc = list(set(string.punctuation))\r\n",
    "\r\n",
    "# Splits words on white spaces (leaves contractions intact) and splits out\r\n",
    "# trailing punctuation\r\n",
    "def casual_tokenizer(text):\r\n",
    "    \"\"\" Called in process_text. \"\"\"\r\n",
    "    tokenizer = TweetTokenizer()\r\n",
    "    tokens = tokenizer.tokenize(text)\r\n",
    "    return tokens\r\n",
    "\r\n",
    "def expand_contractions(text, c_re=c_re):\r\n",
    "    \"\"\" Called in process_text. \"\"\"\r\n",
    "    def replace(match):\r\n",
    "        return c_dict[match.group(0)]\r\n",
    "    return c_re.sub(replace, text)\r\n",
    "\r\n",
    "def process_text(text):\r\n",
    "    text = casual_tokenizer(text)\r\n",
    "    text = [each.lower() for each in text]\r\n",
    "    text = [expand_contractions(each, c_re=c_re) for each in text]\r\n",
    "    text = [re.sub('[0-9]+', '', each) for each in text]\r\n",
    "    text = [re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", each) for each in text]\r\n",
    "    text = [re.sub('[^a-zA-Z]', ' ', each) for each in text]\r\n",
    "    text = [WordNetLemmatizer().lemmatize(each) for each in text]\r\n",
    "    text = [w for w in text if w not in punc]\r\n",
    "    text = [w for w in text if w not in stop_words]\r\n",
    "    text = [each for each in text if len(each) > 1]\r\n",
    "    text = [each for each in text if ' ' not in each]\r\n",
    "    text = unique_words(text)\r\n",
    "    return text\r\n",
    "\r\n",
    "def top_words(topic, n_top_words):\r\n",
    "    return topic.argsort()[:-n_top_words - 1:-1]  \r\n",
    "\r\n",
    "def topic_table(model, feature_names, n_top_words):\r\n",
    "    topics = {}\r\n",
    "    for topic_idx, topic in enumerate(model.components_):\r\n",
    "        t = (topic_idx)\r\n",
    "        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\r\n",
    "    return pd.DataFrame(topics)\r\n",
    "\r\n",
    "def whitespace_tokenizer(text): \r\n",
    "    pattern = r\"(?u)\\b\\w\\w+\\b\" \r\n",
    "    tokenizer_regex = RegexpTokenizer(pattern)\r\n",
    "    tokens = tokenizer_regex.tokenize(text)\r\n",
    "    return tokens\r\n",
    "\r\n",
    "# Funtion to remove duplicate words\r\n",
    "def unique_words(text): \r\n",
    "    ulist = []\r\n",
    "    [ulist.append(x) for x in text if x not in ulist]\r\n",
    "    return ulist\r\n",
    "\r\n",
    "def word_count(text):\r\n",
    "    return len(str(text).split(' '))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Adapted from https://towardsdatascience.com/an-end-to-end-web-service-implementation-for-text-classification-using-word2vec-and-lgbm-3d605c77fd6e\r\n",
    "\r\n",
    "import gensim\r\n",
    "import datetime\r\n",
    "\r\n",
    "VECTOR_SIZE = 150\r\n",
    "\r\n",
    "class WordEmbeddingsService:\r\n",
    "    def __init__(self, raw_text=None, tokenized_text=None):\r\n",
    "        self.raw_text = raw_text\r\n",
    "        self.tokenized_text = tokenized_text\r\n",
    "\r\n",
    "    def set_raw_text(self, raw_text):\r\n",
    "        self.raw_text = raw_text\r\n",
    "\r\n",
    "    def set_tokenized_text(self, tokenized_text):\r\n",
    "        self.tokenized_text = tokenized_text\r\n",
    "\r\n",
    "    def train_w2v_model(self, tokenized_text):\r\n",
    "        self.tokenized_text = tokenized_text\r\n",
    "        model = gensim.models.Word2Vec(tokenized_text,\r\n",
    "                                       window=50,\r\n",
    "                                       vector_size=VECTOR_SIZE,\r\n",
    "                                       epochs=5,\r\n",
    "                                       min_count=3,\r\n",
    "                                       workers=1)\r\n",
    "        return model\r\n",
    "\r\n",
    "    def create_word_embeddings(self, text_list, model):\r\n",
    "        vectors = [self.calculate_avg_vectors(x, model) for x in text_list]\r\n",
    "        vector_series = pd.DataFrame(vectors).apply(pd.Series).reset_index()\r\n",
    "        return vector_series\r\n",
    "\r\n",
    "    def calculate_avg_vectors(self, text_list, model):\r\n",
    "        vectors = []\r\n",
    "        for word in text_list:\r\n",
    "            vectors.append(self.get_vector(word, model))\r\n",
    "        if len(vectors) == 0:\r\n",
    "            return np.zeros(VECTOR_SIZE)\r\n",
    "        vec_avg = np.mean(vectors, axis=0)\r\n",
    "        return vec_avg\r\n",
    "\r\n",
    "    def get_vector(self, word, model):\r\n",
    "        try:\r\n",
    "            return model.wv.get_vector(word)\r\n",
    "        except:\r\n",
    "            return np.zeros(VECTOR_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}